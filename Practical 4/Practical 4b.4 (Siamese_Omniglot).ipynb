{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnW5EMN1zUGU"
   },
   "source": [
    "Code based on: https://sorenbouma.github.io/blog/oneshot/ and https://github.com/sorenbouma/keras-oneshot/blob/master/SiameseNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLhIiFuuzUGV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epDO72-6zUGc"
   },
   "source": [
    "# Siamese Network architecture\n",
    "We define a Siamese Network for use with the Omniglot dataset. The architecture is similar to that in the paper (Koch et al., \"Siamese Neural Networks for One-shot Image Recognition\"), but we include dropout and batch normalization to improve generalization and speed up training.\n",
    "\n",
    "Each siamese \"leg\" is a convnet that transforms data to a 4096-dimensional representation. The metric we are trying to learn is the L1-distance between such representations. The output of the full Siamese Network represents the probability that the two input images are \"similar\" (in this case: of the same class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kYOrfpvzUGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 96, 96, 64)        6464      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9216)              36864     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,985,792\n",
      "Trainable params: 38,966,720\n",
      "Non-trainable params: 19,072\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 4096)         38985792    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(None, 4096)]       0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs (TensorFlowOpLa [(None, 4096)]       0           tf_op_layer_Sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(1, 1)]             0           tf_op_layer_Abs[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_RealDiv (TensorFlow [(None, 4096)]       0           tf_op_layer_Sub[0][0]            \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        tf_op_layer_RealDiv[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 38,989,889\n",
      "Trainable params: 38,970,817\n",
      "Non-trainable params: 19,072\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (105, 105, 1)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "convnet.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(128, (7,7), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(128, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Conv2D(256, (4,4), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "convnet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "class L1_Distance(Layer):\n",
    "    def __init__(self):\n",
    "        super(L1_Distance, self).__init__()\n",
    "    \n",
    "    def __call__(self, left, right):\n",
    "        difference = encoded_l - encoded_r\n",
    "        return tf.linalg.normalize(difference, ord=1)[0] # normalize returns tuple, must specify vector\n",
    "\n",
    "l1_distance = L1_Distance()(encoded_l, encoded_r)\n",
    "prediction = Dense(1, activation=\"sigmoid\")(l1_distance)\n",
    "siamese_net = Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEjVa_kgzUGh"
   },
   "source": [
    "# Omniglot Data\n",
    "We pickled the Omniglot dataset with the \"Practical-4b.3_preprocess-omniglot.ipynb\" notebook, as an array of shape (n_classes x n_examples x width x height), and there is an accompanying dictionary to specify which indexes belong to which languages. Let's load this data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdrp9uCKzUGi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (964, 20, 105, 105)\n",
      "X_test shape: (659, 20, 105, 105)\n",
      "\n",
      "training alphabets\n",
      "['Japanese_(katakana)', 'N_Ko', 'Japanese_(hiragana)', 'Bengali', 'Tagalog', 'Futurama', 'Braille', 'Arcadian', 'Early_Aramaic', 'Korean', 'Grantha', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Tifinagh', 'Greek', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Gujarati', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Syriac_(Estrangelo)', 'Hebrew', 'Anglo-Saxon_Futhorc', 'Asomtavruli_(Georgian)', 'Mkhedruli_(Georgian)', 'Burmese_(Myanmar)', 'Armenian', 'Latin', 'Cyrillic', 'Sanskrit', 'Alphabet_of_the_Magi', 'Malay_(Jawi_-_Arabic)', 'Balinese']\n",
      "test alphabets:\n",
      "['Tibetan', 'Aurek-Besh', 'Keble', 'Oriya', 'Kannada', 'ULOG', 'Syriac_(Serto)', 'Malayalam', 'Atemayar_Qelisayer', 'Manipuri', 'Old_Church_Slavonic_(Cyrillic)', 'Gurmukhi', 'Sylheti', 'Angelic', 'Tengwar', 'Glagolitic', 'Avesta', 'Atlantean', 'Ge_ez', 'Mongolian']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Japanese_(katakana)': [0, 46],\n",
       " 'N_Ko': [47, 79],\n",
       " 'Japanese_(hiragana)': [80, 131],\n",
       " 'Bengali': [132, 177],\n",
       " 'Tagalog': [178, 194],\n",
       " 'Futurama': [195, 220],\n",
       " 'Braille': [221, 246],\n",
       " 'Arcadian': [247, 272],\n",
       " 'Early_Aramaic': [273, 294],\n",
       " 'Korean': [295, 334],\n",
       " 'Grantha': [335, 377],\n",
       " 'Inuktitut_(Canadian_Aboriginal_Syllabics)': [378, 393],\n",
       " 'Tifinagh': [394, 448],\n",
       " 'Greek': [449, 472],\n",
       " 'Blackfoot_(Canadian_Aboriginal_Syllabics)': [473, 486],\n",
       " 'Gujarati': [487, 534],\n",
       " 'Ojibwe_(Canadian_Aboriginal_Syllabics)': [535, 548],\n",
       " 'Syriac_(Estrangelo)': [549, 571],\n",
       " 'Hebrew': [572, 593],\n",
       " 'Anglo-Saxon_Futhorc': [594, 622],\n",
       " 'Asomtavruli_(Georgian)': [623, 662],\n",
       " 'Mkhedruli_(Georgian)': [663, 703],\n",
       " 'Burmese_(Myanmar)': [704, 737],\n",
       " 'Armenian': [738, 778],\n",
       " 'Latin': [779, 804],\n",
       " 'Cyrillic': [805, 837],\n",
       " 'Sanskrit': [838, 879],\n",
       " 'Alphabet_of_the_Magi': [880, 899],\n",
       " 'Malay_(Jawi_-_Arabic)': [900, 939],\n",
       " 'Balinese': [940, 963]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = os.path.join(\"data\" ,\"omniglot\")\n",
    "\n",
    "with open(os.path.join(PATH, \"omniglot_train.p\"), \"rb\") as f:\n",
    "    (X_train, c_train) = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH, \"omniglot_test.p\"), \"rb\") as f:\n",
    "    (X_test, c_test) = pickle.load(f)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"\")\n",
    "print(\"training alphabets\")\n",
    "print([key for key in c_train.keys()])\n",
    "print(\"test alphabets:\")\n",
    "print([key for key in c_test.keys()])\n",
    "\n",
    "c_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2Qhf7YPzUGm"
   },
   "source": [
    "Notice that the training set contains 964 different characters, each belonging to one of 30 languages/scripts. The test set contains 659 different characters, each belonging to one of 20 languages/scripts. Each class (character) has only 20 examples, thus training a classifier on them would likely severely overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4zZkdi-zUGn"
   },
   "source": [
    "# Training functions\n",
    "To be able to train the Siamese Network we need to do a bit more work than for a simple classification network. We cannot simply feed the full dataset into the network batch by batch, instead here we need pairs of examples. These should be positive examples (where both images are from the same class, with a target output of 1) and negative examples (where the two input images are from a different class, with a target output of 0).\n",
    "\n",
    "To achieve this, we define the \"get_batch\" function which selects a number of pairs, half of them with images from the same class, and half of them with images from two different classes.\n",
    "\n",
    "We also define a generator function \"batch_generator\" that will repeatedly generate batches using \"get_batch\", such that we can use it for training with Keras's \"fit_generator\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FX8tG48OzUGo"
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, X):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, 1)) for i in range(2)]\n",
    "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, 1)\n",
    "    return pairs, targets\n",
    "\n",
    "def batch_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit.generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, X)\n",
    "        yield (pairs, targets)\n",
    "\n",
    "def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=1):\n",
    "    model.fit(batch_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2URKOupzzUGs"
   },
   "source": [
    "# One-shot functions\n",
    "The paper aims at using Siamese Networks for N-way One-shot Image Recognition. In this scenario, N examples of reference images are given, each belonging to a different class, as well as a test image that corresponds to exactly one of these classes. The task is to correctly classify which class the test image belongs to, given only one example from each of the N classes.\n",
    "\n",
    "We define a function \"make_oneshot_task\" that can randomly setup such a one-shot task from a given test set (if a language is specified, using only classes/characters from that language), i.e. it will generate N pairs of images, where the first image is always the test image, and the second image is one of the N reference images. The pair of images from the same class will have target 1, all other targets are 0.\n",
    "\n",
    "The function \"test_oneshot\" will generate a number (k) of such one-shot tasks and evaluate the performance of a given model on these tasks; it reports the percentage of correctly classified test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrQjq7klzUGt"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    if language is not None:\n",
    "        low, high = c[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    else:  # if no language specified just pick a bunch of random letters\n",
    "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, 1)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, 1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u69cJf8vzUGx"
   },
   "source": [
    "## Plotting example one-shot tasks\n",
    "Let's visualize some one-shot tasks to get an idea of how well humans can solve such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jA0hckiszUGx"
   },
   "outputs": [],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc,h,w,_ = X.shape\n",
    "    X = X.reshape(nc,h,w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n",
    "    fig,(ax1,ax2) = plt.subplots(2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105),cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXfahYo1zUG1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs, targets = make_oneshot_task(20, X_train, c_train, language=\"Japanese_(katakana)\")\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_XzumAmzUG5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAADuCAYAAADoZyMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAHlklEQVR4nO2dUbKkKBBFYaKXUP3dLqL3v4KqRbzveXuwPyacMRhQgUy5XO+J6J/qpwUcM0EKNK7rGgQPf40ugLBFQsmQUDIklAwJJUNCyZBQMiSUDAkl40fNH79er3VZFqeiiBo+n8/3uq4/08+rhC7LEt7vt12pRDMxxq/c50q5ZEgoGRJKhoSSIaFkSCgZEkqGhJIhoWRIKBkSSoaEkiGhZEgoGRJKhoSSIaFkSCgZEkpG1ZoidGKMl/+WdRvlrUJrGjzlSEDLeffHrOtKczHclnJ7ZJaOjzF2n7d07rO/t/heD24RalX5/XkQGhShDCnuKde60mfnK6VDr8ZPU/doXIWWGrGm4nf3bT199fb/I8W6CbWKiNoBS+25Ucpixe33oS1X75Vj7oqKK98zUrqLUItUa3lsidaGX9f133/W5+7ltgi16t/uHvScgSZ1yqk/y2i1aHSE0e3GlEJLHMkZFUl3R+m0Qr0GV61c6VfvwFwo+rB+RIPf2Sa3RKhXI6L1pSPPvzFtyi1xpeFGp0VP6IT2YDm7NYrpheYa78lROr1Qa9AHdWdQCG2NNu/bmJQ7LhYKoSH8vwFnj7RWaISKf3i8ULbB0eOF3o13VyChjhz91OclVkLJkNAMM4+Qp1qX64XnPePdP6kpQskwF4q25ucqLLcvt0Yo8p4QFlyEnl3tT5NKt2Ihx1Oi1WON8hFuWyG2As+wH8QD2oXWV4ftT4nYEHwvXriV8wxSW9cHW3DrlnyvNDzLRXBHtzLkoRn7ih3JONpM27rx1xq09UvDn4LiEbV3NODVrHD3YG+40I2rYq+QO8dT7o3h5nK9rmjLUTRqdIYAFKF7LKM1xULG2dZ8ymcszAzazrYaoIX2RIk3KAJToIWegf4AixHADYqsqV0xgBp5V5k6QmuYXdRV6CP0aUgoGRJKhoSSIaFkSCgZEkqGhJIhoWRIKBkSSoaEkiGhZEgoGRJKhoSSIaFkSCgZEkqGhJIhoWRIKBkSSoaEkiGhZEgoGRJKhoSSIaFkSCgZEkpGrNk3GWP8O4Tw5VccUcGvdV1/ph9WCRX4KOWSIaFkSCgZEkqGhJIhoWRIKBkSSoaEkiGhZEgoGRJKhoSSIaFkSCgZEkqGhJJR9Yjy1+u1LsviVBRRw+fz+c4tQakSuixLeL/fdqUSzcQYs2u7lHLJkFAy4IQivzgHuWwbTe9tYX0R+hVapMK/EHZ779he7NHbeBGIMXaXq3JR+pB2aE65+1dQpRGL9MZ7pLLcQfersrbo3GR6vvuzljRKRkVNibSNSmWrKTfcoMiK0mubES60PekF15tRIF9md/XKPTs+d1yaUZCwKBNshNa+JrLm2J5ItYzwtJtKv6clWs0itDTibWFfQa+IQovUtByt5eoWij7wOKJFKmI/vKdLaGngMROtkYpaz+Y+lGm2iEVmCB0R6lkp5JSGDtxtC/LVPwOwty2iDQklYyqh6lvPmUboTPe3I4EaFJWkscrsnbPO4RqhFimSVWYI/805W/6O7CZ0E1FTwPTvmWWm5MS2yHVJuT0i9lJnlNmbRtN58VpMhVqLmCVC03KmkVYaF1w9d3reI1x+PrM8j4VUq3OUKGWVoy7Hsn57TISm/V4vXr+H7s9Tc84ro2+Un9VMhB41TE0lSw2H9mN0DgSZIQBNLFxpkJ5G24496td6QbjgYCYWcqM7q2UZoxp6RFaBEbont7Z39NVfM1q9OkDMLd/Z01JnSKE5Rvahpe+t/bzm+1q7F3ehlvtJZp5wuAL8XG4PuYjcpsdQRpRe9GQjyJR7Jow1Qi0yEKTQJ0RhCPl69l6skELRJxEssa4nZB/6FJkeQAoV7UgoGRJKhoSSATfKfco9qMeKvxAAl6DMute0lt4f10uYLkHZbpSRRJwtHfE4/8i6m6fcXqlWM0Rn2cLqe9Lzj57hMhOaLpRqqZh1ZKNkiTsxjdDcUsba489WybVglRYtF6sdATs5b/Xsgt5H0FhGquWqxt41uznMH5qxx6ohe55WMmMKHxah6H3U2bYKpNG4FXATC9bsR925/2MDTqjHoAhZ3JX+csjEghVPmSkKwedC0+Q8GdBCmaPTC2ihoh4JJUNCyZBQQ0b/0hKChJqCsEAc7j7UgpZGRRlR9957Tyn0rNJekxPWEx1HD9po/R6YlJubb7V4shYyuY3N2+et9YYRmtsqaBlZqEDubSltS0doSJS+sYR1G5lF6J3yEEaTFpTq0FM3t5SLsuzjaICxf0iixQVS+zCrEEBT7oZH1PQuDjuSufXbV7/DKjN4rqIwf5KYtdSjx8acHXcmc0/N7YL1DoHSaLcFl4XWvfRWbP/UsCt/W7v4rPc8Ryv96Lbklyqb3s60bnHoeVCUNxbfDyc0h0VU1J67ht5ylLqAFqYQmqO1wtYDGwssM8O0QnuwaMDR6bkEzNSfsEFCyYg1qSPGiJlnnslnXdff6YeKUDIklAwJJUNCyZBQMiSUDAklQ0LJkFAyJJQMCSVDQsmQUDIklAwJJUNCyZBQMiSUDAklo3YZ53cI4cujIKKaX7kPqxaJCXyUcsmQUDIklAwJJUNCyZBQMiSUDAklQ0LJ+APAlB5sKdDOyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs, targets = make_oneshot_task(20, X_train, c_train)\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9wuZ4l5zUHF"
   },
   "source": [
    "# Training\n",
    "Let's train the model now. In each training loop, we sample 100 batches of 64 image pairs (as specified in the \"train\" method above), after which we evaluate the one-shot image recognition accuracy of the model. Whenever the model achieves a new best accuracy, we save its weights to a file (note that we do not directly use the value of the loss function).\n",
    "\n",
    "*NOTE: training may take a long time, especially if training on CPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEeM38DSzUHG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training loop 1 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.0862\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 2.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 2 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6935\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 3.6% accuracy for 20-way one-shot learning\n",
      "=== Training loop 3 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6936\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 6.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 4 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6936\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 4.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 5 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6937\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 4.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 6 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6938\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 7.2% accuracy for 20-way one-shot learning\n",
      "=== Training loop 7 ===\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.6938\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 4.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 8 ===\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.6939\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 4.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 9 ===\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.6938\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 2.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 10 ===\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.6936\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 3.2% accuracy for 20-way one-shot learning\n"
     ]
    }
   ],
   "source": [
    "loops = 10\n",
    "best_acc = 0\n",
    "for i in range(loops):\n",
    "    print(\"=== Training loop {} ===\".format(i+1))\n",
    "    # === ADD CODE HERE ===\n",
    "    train(siamese_net, X_train)\n",
    "    test_oneshot(siamese_net, X_test, c_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical4b.4 (Siamese_Omniglot)_a.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
