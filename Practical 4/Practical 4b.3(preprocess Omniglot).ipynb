{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gc_KLcGCNsyy"
   },
   "source": [
    "Code based on: https://github.com/sorenbouma/keras-oneshot/blob/master/load_data.py\n",
    "\n",
    "This script preprocesses the omniglot dataset (training and test set, stored as PNG files in subfolders) into a numpy array of shape (n_classes, n_examples, n_drawings, width, height), as well as a dictionary that can be referenced to find out which alphabet a particular (character) class belongs to. This data is then stored as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQCfBHsLNsyz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imageio import imread\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-Hsr4qNNsy6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: Braille\n",
      "loading alphabet: Arcadian\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Korean\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Tifinagh\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Gujarati\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: ULOG\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Angelic\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Mongolian\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to preprocess the omniglot dataset and pickle it into an array that's easy to index by character type\"\"\"\n",
    "data_path = os.path.join(\"data\", \"omniglot\")\n",
    "train_path = os.path.join(data_path, \"omniglot_train\")\n",
    "test_path = os.path.join(data_path,\"omniglot_test\")\n",
    "\n",
    "lang_dict = {}\n",
    "\n",
    "def loadimgs(path, n=0):\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    # we load every alphabet separately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        # every letter/category has its own column in the array, so load separately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            #edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X, y, lang_dict\n",
    "\n",
    "X, y, c = loadimgs(train_path)\n",
    "with open(os.path.join(data_path, \"omniglot_train.p\"), \"wb\") as f:\n",
    "    pickle.dump((X, c), f)\n",
    "\n",
    "X, y, c = loadimgs(test_path)\n",
    "with open(os.path.join(data_path, \"omniglot_test.p\"), \"wb\") as f:\n",
    "    pickle.dump((X, c), f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical 4b.3(preprocess Omniglot).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
