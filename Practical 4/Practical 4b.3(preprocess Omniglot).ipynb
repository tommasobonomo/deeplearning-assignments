{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Practical 4b.3(preprocess Omniglot).ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gc_KLcGCNsyy","colab_type":"text"},"source":["Code based on: https://github.com/sorenbouma/keras-oneshot/blob/master/load_data.py\n","\n","This script preprocesses the omniglot dataset (training and test set, stored as PNG files in subfolders) into a numpy array of shape (n_classes, n_examples, n_drawings, width, height), as well as a dictionary that can be referenced to find out which alphabet a particular (character) class belongs to. This data is then stored as pickle files."]},{"cell_type":"code","metadata":{"id":"sQCfBHsLNsyz","colab_type":"code","colab":{}},"source":["import numpy as np\n","from imageio import imread\n","import pickle\n","import os\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qD3xtYr6N0J0","colab_type":"code","colab":{}},"source":["# mount the data needed to drive folder so we can use them in colab, see the data download link in Practical 4a.1\n","from google.colab import drive\n","!mkdir drive\n","drive.mount('drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxBbHzH3N0Ws","colab_type":"code","colab":{}},"source":["#  list all the data in your drive folder to see if mount successfully.\n","!ls \"drive/My Drive/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-Hsr4qNNsy6","colab_type":"code","colab":{}},"source":["\"\"\"Script to preprocess the omniglot dataset and pickle it into an array that's easy to index by character type\"\"\"\n","data_path = os.path.join(\"drive\",\"My Drive\",\"data_DL_practical\", \"omniglot\")\n","train_path = os.path.join(data_path, \"omniglot_train\")\n","test_path = os.path.join(data_path,\"omniglot_test\")\n","\n","lang_dict = {}\n","\n","def loadimgs(path, n=0):\n","    X=[]\n","    y = []\n","    cat_dict = {}\n","    lang_dict = {}\n","    curr_y = n\n","    # we load every alphabet separately so we can isolate them later\n","    for alphabet in os.listdir(path):\n","        print(\"loading alphabet: \" + alphabet)\n","        lang_dict[alphabet] = [curr_y,None]\n","        alphabet_path = os.path.join(path,alphabet)\n","        # every letter/category has its own column in the array, so load separately\n","        for letter in os.listdir(alphabet_path):\n","            cat_dict[curr_y] = (alphabet, letter)\n","            category_images=[]\n","            letter_path = os.path.join(alphabet_path, letter)\n","            for filename in os.listdir(letter_path):\n","                image_path = os.path.join(letter_path, filename)\n","                image = imread(image_path)\n","                category_images.append(image)\n","                y.append(curr_y)\n","            try:\n","                X.append(np.stack(category_images))\n","            #edge case  - last one\n","            except ValueError as e:\n","                print(e)\n","                print(\"error - category_images:\", category_images)\n","            curr_y += 1\n","            lang_dict[alphabet][1] = curr_y - 1\n","    y = np.vstack(y)\n","    X = np.stack(X)\n","    return X, y, lang_dict\n","\n","X, y, c = loadimgs(train_path)\n","with open(os.path.join(data_path, \"omniglot_train.p\"), \"wb\") as f:\n","    pickle.dump((X, c), f)\n","\n","X, y, c = loadimgs(test_path)\n","with open(os.path.join(data_path, \"omniglot_test.p\"), \"wb\") as f:\n","    pickle.dump((X, c), f)"],"execution_count":0,"outputs":[]}]}