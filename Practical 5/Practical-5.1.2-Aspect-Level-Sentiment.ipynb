{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0VnIJhDRpvc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32945,
     "status": "ok",
     "timestamp": 1589893936395,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "KD5kpfgB1o_z",
    "outputId": "48d05a82-a482-4935-891b-dca5e271e6b3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJ7e_DyF7ka-"
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4V9hW-r7kbF"
   },
   "outputs": [],
   "source": [
    "aspect_path = 'data/aspect-level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl50euzvDcQP"
   },
   "outputs": [],
   "source": [
    "### Reading preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6rbh26b7kbe"
   },
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj5mi_qW7kbj"
   },
   "outputs": [],
   "source": [
    "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
    "\n",
    "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
    "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
    "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
    "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
    "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
    "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
    "\n",
    "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
    "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
    "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')\n",
    "\n",
    "\n",
    "pretrain_data = read_pickle(aspect_path, 'pretrain_data.pkl')\n",
    "pretrain_label = read_pickle(aspect_path, 'pretrain_label.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adNP5FHo7kbr"
   },
   "source": [
    "### Batch generator and data iterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6PrfXAb7kbt"
   },
   "outputs": [],
   "source": [
    "class Dataiterator():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, aspect_data, doc_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
    "        \n",
    "        len_aspect_data = len(aspect_data[0])\n",
    "        len_doc_data = len(doc_data[0])\n",
    "        \n",
    "        self.X_aspect = aspect_data[0] \n",
    "        self.y_aspect = aspect_data[1]\n",
    "        self.aspect_terms = aspect_data[2]\n",
    "        \n",
    "        self.X_doc = doc_data[0]\n",
    "        self.y_doc = doc_data[1]\n",
    "        \n",
    "        self.num_data = len_aspect_data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
    "        batch_X_doc = self.X_doc[np.array(X_ids)]\n",
    "        batch_y_doc = self.y_doc[np.array(X_ids)]\n",
    "        \n",
    "        \n",
    "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms, batch_X_doc, batch_y_doc\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X_aspect, self.y_aspect, self.aspect_terms, self.X_doc, self.y_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sklA6jSUWDS6"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9411,
     "status": "ok",
     "timestamp": 1589873378907,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "vDU0hXVJ1PR5",
    "outputId": "3cdcd784-7aa1-49c0-9a2e-534745469769"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM\n",
    "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pvg-Uf2h_6Qm"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dnbn0_O0XlGf"
   },
   "source": [
    "### Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfwhzqK97kb9"
   },
   "outputs": [],
   "source": [
    "overal_maxlen = 82\n",
    "overal_maxlen_aspect = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjiAsayW1zx9"
   },
   "outputs": [],
   "source": [
    "class Custom_softmax(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Custom_softmax, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        if self.mask_zero:\n",
    "            a = K.exp(x)         \n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            a = a * mask\n",
    "            a=a / (K.sum(a, axis=1, keepdims=True) + K.epsilon())\n",
    "            return a\n",
    "        else:\n",
    "            return K.softmax(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1],1)\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bp5xh27GXmva"
   },
   "outputs": [],
   "source": [
    "repeator = RepeatVector(overal_maxlen, name='repeator_att')\n",
    "concatenator = Concatenate(axis=-1, name='concator_att')\n",
    "densor1 = Dense(300, activation = \"tanh\", name='densor1_att')\n",
    "densor2 = Dense(1, activation = \"relu\", name='densor2_att')\n",
    "activator = Custom_softmax(mask_zero=True,name='softmax_att')\n",
    "dotor = Dot(axes = 1, name='dotor_att')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVz3DuBbU0yG"
   },
   "source": [
    "keys shape:(batch_size,overal_maxlen,out_len_lstm)\n",
    "Query shape:(batch_size,out_len_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VH3g4z6R8O3_"
   },
   "outputs": [],
   "source": [
    "########################################################################################################################################\n",
    "#### Shape of Keys:[batch-size, overal-maxlen,Dimension_output_lstm], Shape of query: [batch-size,1,Dimension-output-wordEmbedding]  ###\n",
    "########################################################################################################################################\n",
    "def attention(keys, query):\n",
    "    query = repeator(query)\n",
    "    print(\"query shape: %s\" %str(query._keras_shape))\n",
    "    concat = concatenator([keys, query])\n",
    "    print(\"concat shape: %s\" %str(concat._keras_shape))\n",
    "    e1 = densor1(concat)\n",
    "    print(\"e1 shape: %s\" %str(e1._keras_shape))\n",
    "    e2 = densor2(e1)\n",
    "    print(\"e2 shape: %s\" %str(e2._keras_shape))\n",
    "    alphas = activator(e2)\n",
    "    print(\"alphas shape: %s\" %str(alphas._keras_shape))\n",
    "    context = dotor([alphas, keys])\n",
    "    print(\"context shape: %s\" %str(context._keras_shape))\n",
    "    \n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVmvIn0X_0Ji"
   },
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mask_zero:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
    "        else:\n",
    "            return K.mean(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsyFHokczuvO"
   },
   "source": [
    "### Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpjtVz0g7kcf"
   },
   "outputs": [],
   "source": [
    "dropout = 0.5     \n",
    "recurrent_dropout = 0.1\n",
    "vocab_size = len(vocab)\n",
    "num_outputs = 3 # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvUi2LZu7kcj"
   },
   "source": [
    "### Inputs: How many inputs do you need for the current task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n--aInYI7kck"
   },
   "outputs": [],
   "source": [
    "##### Inputs #####\n",
    "sentence_input = Input(shape=(overal_maxlen,), dtype='int32', name='sentence_input')\n",
    "aspect_input = Input(shape=(overal_maxlen_aspect,), dtype='int32', name='aspect_input')\n",
    "pretrain_input = Input(shape=(None,), dtype='int32', name='pretrain_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMS-elf-x4UU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAj0NHwX7kcr"
   },
   "source": [
    "### Word-level embedding (shareable between all model inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xktj5dB77kcs"
   },
   "outputs": [],
   "source": [
    "##### construct word embedding layer #####\n",
    "word_emb = Embedding(vocab_size, 300, mask_zero=True, name='word_emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DoLxX_f7kcw"
   },
   "source": [
    "### Aspect-level representation (averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1589837918719,
     "user": {
      "displayName": "tianjin huang",
      "photoUrl": "",
      "userId": "00543616640831635256"
     },
     "user_tz": -120
    },
    "id": "ULXduGya7kcx",
    "outputId": "7fa0a202-8768-4d6b-a9a7-e95ee0274dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use average term embs as aspect embedding\n"
     ]
    }
   ],
   "source": [
    "### represent aspect as averaged word embedding ###\n",
    "print ('use average term embs as aspect embedding')\n",
    "aspect_embs_overall = word_emb(aspect_input) #get the embedding representation for asepct terms\n",
    "aspect_embs = Average()(aspect_embs_overall)# average it Using Average  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DpH3jWF7kc6"
   },
   "source": [
    "### Sentence-level representation from two domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXNovw_A7kc8"
   },
   "outputs": [],
   "source": [
    "### sentence representation ###\n",
    "sentence_embs= word_emb(sentence_input) # from aspect-level domain\n",
    "pretrain_embs= word_emb(pretrain_input) # from document-level domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVCkJt6v7kdD"
   },
   "source": [
    "### LSTM layer (shared between three representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meql_fR07kdE"
   },
   "outputs": [],
   "source": [
    "rnn = LSTM(300, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout, name='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx18YY_I7kdH"
   },
   "outputs": [],
   "source": [
    "sentence_lstm = rnn(sentence_embs)  #sentence from aspect-level\n",
    "pretrain_lstm = rnn(pretrain_embs)  #sentence from document-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jktzwmPI7kdO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (None, 82, 300)\n",
      "concat shape: (None, 82, 600)\n",
      "e1 shape: (None, 82, 300)\n",
      "e2 shape: (None, 82, 1)\n",
      "alphas shape: (None, 82, 1)\n",
      "context shape: (None, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT REPLACE KEYS?, QUERY? WITH THE CORRESPONDING TENSORS AS ATTENTION KEYS AND QUERY\n",
    "att_context, att_weights = attention(sentence_lstm, aspect_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7US3khfR7kdZ"
   },
   "outputs": [],
   "source": [
    "pretrain_avg = Average(mask_zero=True)(pretrain_lstm)\n",
    "\n",
    "sentence_output = Dense(num_outputs, name='dense_1')(att_context)\n",
    "pretrain_output = Dense(num_outputs, name='dense_2')(pretrain_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-jE8dhm7kde"
   },
   "outputs": [],
   "source": [
    "sentence_output = Reshape((num_outputs,))(sentence_output)   #Shape (Batch-size,1,3)->(Batch-size,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dgKnXfO7kdh"
   },
   "outputs": [],
   "source": [
    "aspect_probs = Activation('softmax', name='aspect_model')(sentence_output)\n",
    "doc_probs = Activation('softmax', name='pretrain_model')(pretrain_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6ptr7iuWIYT"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[sentence_input, aspect_input, pretrain_input], outputs=[aspect_probs, doc_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmPSF8IAENj_"
   },
   "outputs": [],
   "source": [
    "import keras.optimizers as opt\n",
    "\n",
    "optimizer = opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "eiFkEy-eVtoU",
    "outputId": "8e611845-9951-4f35-bcba-a55103743742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sentence_input (InputLayer)     (None, 82)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "aspect_input (InputLayer)       (None, 7)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_emb (Embedding)            multiple             3000900     aspect_input[0][0]               \n",
      "                                                                 sentence_input[0][0]             \n",
      "                                                                 pretrain_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_1 (Average)             (None, 300)          0           word_emb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     multiple             721200      word_emb[1][0]                   \n",
      "                                                                 word_emb[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeator_att (RepeatVector)     (None, 82, 300)      0           average_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concator_att (Concatenate)      (None, 82, 600)      0           lstm[0][0]                       \n",
      "                                                                 repeator_att[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "densor1_att (Dense)             (None, 82, 300)      180300      concator_att[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "densor2_att (Dense)             (None, 82, 1)        301         densor1_att[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pretrain_input (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "softmax_att (Custom_softmax)    (None, 82, 1)        0           densor2_att[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dotor_att (Dot)                 (None, 1, 300)       0           softmax_att[0][0]                \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 3)         903         dotor_att[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_2 (Average)             (None, 300)          0           lstm[1][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            903         average_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "aspect_model (Activation)       (None, 3)            0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pretrain_model (Activation)     (None, 3)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,904,507\n",
      "Trainable params: 3,904,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FaNcPdHiC650"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss={'aspect_model': 'categorical_crossentropy', 'pretrain_model': 'categorical_crossentropy'},\n",
    "              loss_weights = {'aspect_model': 1, 'pretrain_model': 0.1},\n",
    "              metrics = {'aspect_model': 'categorical_accuracy', 'pretrain_model': 'categorical_accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilIKUGn_DEgB"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi0coTFsDWFG"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y3_eO4QgyC3"
   },
   "outputs": [],
   "source": [
    "train_steps_epoch = len(train_x)/batch_size\n",
    "batch_train_iter = Dataiterator([train_x, train_y, train_aspect], \\\n",
    "                                [pretrain_data, pretrain_label], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcKpKWBm7keF"
   },
   "outputs": [],
   "source": [
    "val_steps_epoch = len(dev_x)/batch_size\n",
    "batch_val_iter = Dataiterator([dev_x, dev_y, dev_aspect], \\\n",
    "                              [pretrain_data, pretrain_label], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXqu6Ho47keJ"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_generator(model, batch_train_iter, batch_val_iter):\n",
    "    \n",
    "    earlystop_callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                     ModelCheckpoint(filepath=os.path.join('./','{epoch:02d}-{loss:.2f}.check'), \\\n",
    "                                     monitor='val_loss', save_best_only=False, \\\n",
    "                                     save_weights_only=True)\n",
    "                     ]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[[X, aspect, pretrain_X], [y, pretrain_y]] for X, y, \\\n",
    "                             aspect, pretrain_X, pretrain_y in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            val_batches = [[[X, aspect, pretrain_X], [y, pretrain_y]] for X, y, \\\n",
    "                           aspect, pretrain_X, pretrain_y in batch_val_iter]\n",
    "            for val_batch in val_batches:\n",
    "                yield val_batch\n",
    "                \n",
    "    history = model.fit_generator(train_gen(), validation_data=val_gen(), \\\n",
    "                                  validation_steps=val_steps_epoch, steps_per_epoch=train_steps_epoch, \\\n",
    "                                  epochs = 20, callbacks = earlystop_callbacks)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxAjkoqz7keO",
    "outputId": "83e27987-0134-4b69-d02f-7c385d5eda0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "58/57 [==============================] - 66s 1s/step - loss: 1.0409 - aspect_model_loss: 0.9642 - pretrain_model_loss: 0.7671 - aspect_model_categorical_accuracy: 0.5625 - pretrain_model_categorical_accuracy: 0.7775 - val_loss: 1.1020 - val_aspect_model_loss: 0.8733 - val_pretrain_model_loss: 0.6860 - val_aspect_model_categorical_accuracy: 0.5938 - val_pretrain_model_categorical_accuracy: 0.7812\n",
      "Epoch 2/20\n",
      "58/57 [==============================] - 63s 1s/step - loss: 0.7514 - aspect_model_loss: 0.6901 - pretrain_model_loss: 0.6133 - aspect_model_categorical_accuracy: 0.7263 - pretrain_model_categorical_accuracy: 0.7829 - val_loss: 0.8852 - val_aspect_model_loss: 0.8600 - val_pretrain_model_loss: 0.5827 - val_aspect_model_categorical_accuracy: 0.6333 - val_pretrain_model_categorical_accuracy: 0.7937\n",
      "Epoch 3/20\n",
      "58/57 [==============================] - 61s 1s/step - loss: 0.6493 - aspect_model_loss: 0.5942 - pretrain_model_loss: 0.5514 - aspect_model_categorical_accuracy: 0.7613 - pretrain_model_categorical_accuracy: 0.7909 - val_loss: 1.1914 - val_aspect_model_loss: 0.9224 - val_pretrain_model_loss: 0.4667 - val_aspect_model_categorical_accuracy: 0.6208 - val_pretrain_model_categorical_accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.5580 - aspect_model_loss: 0.5071 - pretrain_model_loss: 0.5084 - aspect_model_categorical_accuracy: 0.8006 - pretrain_model_categorical_accuracy: 0.8077 - val_loss: 0.8361 - val_aspect_model_loss: 0.9781 - val_pretrain_model_loss: 0.4604 - val_aspect_model_categorical_accuracy: 0.5854 - val_pretrain_model_categorical_accuracy: 0.8313\n",
      "Epoch 5/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.5095 - aspect_model_loss: 0.4609 - pretrain_model_loss: 0.4859 - aspect_model_categorical_accuracy: 0.8270 - pretrain_model_categorical_accuracy: 0.8265 - val_loss: 0.9427 - val_aspect_model_loss: 0.9761 - val_pretrain_model_loss: 0.4363 - val_aspect_model_categorical_accuracy: 0.5875 - val_pretrain_model_categorical_accuracy: 0.8354\n",
      "Epoch 6/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.4646 - aspect_model_loss: 0.4195 - pretrain_model_loss: 0.4512 - aspect_model_categorical_accuracy: 0.8475 - pretrain_model_categorical_accuracy: 0.8448 - val_loss: 1.0051 - val_aspect_model_loss: 0.9722 - val_pretrain_model_loss: 0.4038 - val_aspect_model_categorical_accuracy: 0.6458 - val_pretrain_model_categorical_accuracy: 0.8458\n",
      "Epoch 7/20\n",
      "58/57 [==============================] - 61s 1s/step - loss: 0.4412 - aspect_model_loss: 0.3976 - pretrain_model_loss: 0.4357 - aspect_model_categorical_accuracy: 0.8481 - pretrain_model_categorical_accuracy: 0.8384 - val_loss: 1.1062 - val_aspect_model_loss: 1.1382 - val_pretrain_model_loss: 0.3915 - val_aspect_model_categorical_accuracy: 0.5854 - val_pretrain_model_categorical_accuracy: 0.8396\n",
      "Epoch 8/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.4029 - aspect_model_loss: 0.3635 - pretrain_model_loss: 0.3939 - aspect_model_categorical_accuracy: 0.8615 - pretrain_model_categorical_accuracy: 0.8508 - val_loss: 1.5583 - val_aspect_model_loss: 1.0021 - val_pretrain_model_loss: 0.3542 - val_aspect_model_categorical_accuracy: 0.6438 - val_pretrain_model_categorical_accuracy: 0.8646\n",
      "Epoch 9/20\n",
      "58/57 [==============================] - 63s 1s/step - loss: 0.3853 - aspect_model_loss: 0.3468 - pretrain_model_loss: 0.3850 - aspect_model_categorical_accuracy: 0.8723 - pretrain_model_categorical_accuracy: 0.8610 - val_loss: 1.1165 - val_aspect_model_loss: 1.0583 - val_pretrain_model_loss: 0.3997 - val_aspect_model_categorical_accuracy: 0.5875 - val_pretrain_model_categorical_accuracy: 0.8229\n",
      "Epoch 10/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.3596 - aspect_model_loss: 0.3238 - pretrain_model_loss: 0.3578 - aspect_model_categorical_accuracy: 0.8804 - pretrain_model_categorical_accuracy: 0.8675 - val_loss: 1.3423 - val_aspect_model_loss: 1.1845 - val_pretrain_model_loss: 0.3122 - val_aspect_model_categorical_accuracy: 0.6187 - val_pretrain_model_categorical_accuracy: 0.8646\n",
      "Epoch 11/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.3488 - aspect_model_loss: 0.3152 - pretrain_model_loss: 0.3369 - aspect_model_categorical_accuracy: 0.8836 - pretrain_model_categorical_accuracy: 0.8702 - val_loss: 0.9018 - val_aspect_model_loss: 1.1533 - val_pretrain_model_loss: 0.3099 - val_aspect_model_categorical_accuracy: 0.6208 - val_pretrain_model_categorical_accuracy: 0.8729\n",
      "Epoch 12/20\n",
      "58/57 [==============================] - 60s 1s/step - loss: 0.3320 - aspect_model_loss: 0.2978 - pretrain_model_loss: 0.3422 - aspect_model_categorical_accuracy: 0.8928 - pretrain_model_categorical_accuracy: 0.8761 - val_loss: 1.1113 - val_aspect_model_loss: 1.1671 - val_pretrain_model_loss: 0.2721 - val_aspect_model_categorical_accuracy: 0.5833 - val_pretrain_model_categorical_accuracy: 0.8896\n",
      "Epoch 13/20\n",
      "58/57 [==============================] - 61s 1s/step - loss: 0.3046 - aspect_model_loss: 0.2748 - pretrain_model_loss: 0.2981 - aspect_model_categorical_accuracy: 0.8912 - pretrain_model_categorical_accuracy: 0.8847 - val_loss: 1.2227 - val_aspect_model_loss: 1.2128 - val_pretrain_model_loss: 0.3088 - val_aspect_model_categorical_accuracy: 0.5958 - val_pretrain_model_categorical_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "58/57 [==============================] - 61s 1s/step - loss: 0.2964 - aspect_model_loss: 0.2681 - pretrain_model_loss: 0.2832 - aspect_model_categorical_accuracy: 0.8971 - pretrain_model_categorical_accuracy: 0.8944 - val_loss: 1.0052 - val_aspect_model_loss: 1.2574 - val_pretrain_model_loss: 0.2568 - val_aspect_model_categorical_accuracy: 0.5833 - val_pretrain_model_categorical_accuracy: 0.8813\n"
     ]
    }
   ],
   "source": [
    "train_generator(model, batch_train_iter, batch_val_iter)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Practical-5.1.2-Aspect-Level-Sentiment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
