{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2IMM10_Ass2_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YekFjUYf17dU",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2, Question 1\n",
        "\n",
        "<b>Group [fill in group number]</b>\n",
        "* <b> Student 1 </b> : FILL IN STUDENT NAME + STUDENT NUMBER\n",
        "* <b> Student 2 </b> : FILL IN STUDENT NAME + STUDENT NUMBER\n",
        "\n",
        "**Reading material**\n",
        "* [1] *Artem Babenko, Anton Slesarev, Alexandr Chigorin, Victor Lempitsky, \"Neural Codes for Image Retrieval\"*, ECCV, 2014. https://arxiv.org/abs/1404.1777.\n",
        "\n",
        "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MadhDA98yGwO",
        "colab_type": "text"
      },
      "source": [
        "# Image Retrieval with Neural Codes\n",
        "\n",
        "In this task, we are trying the approach proposed in [1], meaning we are using the representations learned by a ConvNet for image retrieval. In particular, we are going to \n",
        "\n",
        "\n",
        "1.   Train and evaluate a ConvNet on an image dataset.\n",
        "2.   Compute the outputs of intermediate layers for a new image dataset, which has not been used during training. These values serve as a representation, so-called *neural codes* for the new images.\n",
        "3.   Use the neural codes for image retrieval, by comparing the Euclidean distances between the codes of a query image and the remaining images.\n",
        "4.   Evaluate results both qualitatively and in terms of *mean average precision*.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4DPPela2BHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import multiprocessing\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Flatten, Input, Dense, Conv2D, MaxPooling2D, ReLU, Dropout, Reshape, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras import losses, optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import time\n",
        "\n",
        "num_train_classes = 190"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GmvHQx_PHJ",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive\n",
        "We will save our model there, in the folder deeplearning2020_ass2_task1.\n",
        "**The model is rather big, so please make sure you have about 1 GB of space in your Google Drive.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r91RRLkt_DG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir('drive'):\n",
        "  drive.mount('drive')\n",
        "else:\n",
        "  print('drive already mounted')\n",
        "\n",
        "base_path = os.path.join('drive', 'My Drive', 'deeplearning2020_ass2_task1')\n",
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTGW9aDM1oWk",
        "colab_type": "text"
      },
      "source": [
        "# Download Tiny Imagenet\n",
        "Tiny Imagenet is small subset of the original Imagenet dataset (http://www.image-net.org/), which is one of the most important large scale image classification datasets.\n",
        "Tiny Imagenet has 200 classes, and contains 500 training examples for each class, i.e. 100,000 training examples in total. The images are of dimensions 64x64. \n",
        "**Note: You will need to re-download the data, when your Colab session has been disconnected (i.e. re-evaluate this cell).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rPrDQzR3NF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tiny imagenet\n",
        "if not os.path.isdir('tiny-imagenet-200'):\n",
        "  start_time = time.time()\n",
        "  if not os.path.isfile('tiny-imagenet-200.zip'):\n",
        "    ! wget \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "  ! unzip -q tiny-imagenet-200.zip -d .\n",
        "  print(\"Unzipped.\")\n",
        "  print(\"Elapsed time: {} seconds.\".format(time.time()-start_time))\n",
        "else:\n",
        "  print('Found folder tiny-imagenet-200')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YtWcQRy3byQ",
        "colab_type": "text"
      },
      "source": [
        "# Load Tiny Imagenet\n",
        "We are going to use the official training set of Tiny Imagenet for our purposes, and ignore the official validation and test sets. \n",
        "We will use 190 classes (95,000 images) of the official training set to make new training, validation and test sets, containing 76,000, 9,500, and 9,500 images, respectively. The remaining 10 classes (5,000 images) will never have been seen during training, and will constitute an *out-of-domain* (ood) set.\n",
        "The ood set will be used for image retrieval.\n",
        "Thus, we'll have:\n",
        "\n",
        "*   Train data: 76,000 images, 64x64x3 pixels, classes 0-189\n",
        "*   Validation data: 9,500 images, 64x64x3 pixels, classes 0-189\n",
        "*   Test data: 9,500 images, 64x64x3 pixels, classes 0-189\n",
        "*   Out-of-domain data: 5000 images, 64x64x3 pixels, **classes 190-199**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLEs8-p4BuLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_imagenet(num_train_classes):\n",
        "  def load_class_images(class_string, label):\n",
        "    \"\"\"\n",
        "    Loads all images in folder class_string.\n",
        "\n",
        "    :param class_string: image folder (e.g. 'n01774750')\n",
        "    :param label: label to be assigned to these images\n",
        "    :return class_k_img: (num_files, width, height, 3) numpy array containing\n",
        "                         images of folder class_string\n",
        "    :return class_k_labels: numpy array containing labels\n",
        "    \"\"\"\n",
        "    class_k_path = os.path.join('tiny-imagenet-200/train/', class_string, 'images')\n",
        "    file_list = sorted(os.listdir(class_k_path))\n",
        "\n",
        "    dtype = np.uint8\n",
        "\n",
        "    class_k_img = np.zeros((len(file_list), 64, 64, 3), dtype=dtype)\n",
        "    for l, f in enumerate(file_list):\n",
        "      file_path = os.path.join('tiny-imagenet-200/train/', class_string, 'images', f)\n",
        "      img = mpimg.imread(file_path)\n",
        "      if len(img.shape) == 2:\n",
        "        class_k_img[l, :, :, :] = np.expand_dims(img, -1).astype(dtype)\n",
        "      else:\n",
        "        class_k_img[l, :, :, :] = img.astype(dtype)\n",
        "\n",
        "    class_k_labels = label * np.ones(len(file_list), dtype=dtype)\n",
        "\n",
        "    return class_k_img, class_k_labels\n",
        "\n",
        "  # get the word description for all imagenet 82115 classes\n",
        "  all_class_dict = {}\n",
        "  for k, line in enumerate(open('tiny-imagenet-200/words.txt', 'r')):\n",
        "    n_id, description = line.split('\\t')[:2]\n",
        "    all_class_dict[n_id] = description\n",
        "\n",
        "  # this will be the description for our 200 classes\n",
        "  class_dict = {}\n",
        "\n",
        "  # we enumerate the classes according to their folder names:\n",
        "  # 'n01443537' -> 0\n",
        "  # 'n01629819' -> 1\n",
        "  # ...\n",
        "  ls_train = sorted(os.listdir('tiny-imagenet-200/train'))\n",
        "  img = None\n",
        "  labels = None\n",
        "  ood_x = None\n",
        "  ood_y = None\n",
        "\n",
        "  # the first num_train_classes will make the training, validation, test sets\n",
        "  for k in range(num_train_classes):\n",
        "    # the word descritpion of the current class\n",
        "    class_dict[k] = all_class_dict[ls_train[k]]\n",
        "    # load images and labels for current class\n",
        "    class_k_img, class_k_labels = load_class_images(ls_train[k], k)\n",
        "    # concatenate all samples and labels\n",
        "    if img is None:\n",
        "      img = class_k_img\n",
        "      labels = class_k_labels\n",
        "    else:\n",
        "      img = np.concatenate((img, class_k_img), axis=0)\n",
        "      labels = np.concatenate((labels, class_k_labels))\n",
        "\n",
        "  # the remaining classes are the out of domain (ood) set \n",
        "  for k in range(num_train_classes, 200):\n",
        "    class_dict[k] = all_class_dict[ls_train[k]]\n",
        "    class_k_img, class_k_labels = load_class_images(ls_train[k], k)\n",
        "    if ood_x is None:\n",
        "      ood_x = class_k_img\n",
        "      ood_y = class_k_labels\n",
        "    else:\n",
        "      ood_x = np.concatenate((ood_x, class_k_img), axis=0)\n",
        "      ood_y = np.concatenate((ood_y, class_k_labels))\n",
        "\n",
        "  return img, labels, ood_x, ood_y, class_dict\n",
        "\n",
        "print('Loading data...')\n",
        "start_time = time.time()\n",
        "train_x, train_y, ood_x, ood_y, class_dict = load_imagenet(num_train_classes)\n",
        "print('Data loaded in {} seconds.'.format(time.time() - start_time))\n",
        "\n",
        "def split_data(x, y, N):\n",
        "  x_N = x[0:N, ...]\n",
        "  y_N = y[0:N]\n",
        "  x_rest = x[N:, ...]\n",
        "  y_rest = y[N:, ...]\n",
        "  return x_N, y_N, x_rest, y_rest\n",
        "\n",
        "# fix random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# shuffle\n",
        "N = train_x.shape[0]\n",
        "rp = np.random.permutation(N)\n",
        "train_x = train_x[rp, ...]\n",
        "train_y = train_y[rp]\n",
        "\n",
        "# train/validation split 80 - 10 - 10\n",
        "N_val = int(round(N * 0.1))\n",
        "N_test = int(round(N * 0.1))\n",
        "val_x, val_y, train_x, train_y = split_data(train_x, train_y, N_val)\n",
        "test_x, test_y, train_x, train_y = split_data(train_x, train_y, N_test)\n",
        "\n",
        "# shuffle ood data\n",
        "N_ood = ood_x.shape[0]\n",
        "rp = np.random.permutation(N_ood)\n",
        "ood_x = ood_x[rp, ...]\n",
        "ood_y = ood_y[rp]\n",
        "\n",
        "# convert all data into float32\n",
        "train_x = train_x.astype(np.float32)\n",
        "train_y = train_y.astype(np.float32)\n",
        "val_x = val_x.astype(np.float32)\n",
        "val_y = val_y.astype(np.float32)\n",
        "test_x = test_x.astype(np.float32)\n",
        "test_y = test_y.astype(np.float32)\n",
        "ood_x = ood_x.astype(np.float32)\n",
        "ood_y = ood_y.astype(np.float32)\n",
        "\n",
        "# normalize\n",
        "train_x /= 255.\n",
        "val_x /= 255.\n",
        "test_x /= 255.\n",
        "ood_x /= 255.\n",
        "\n",
        "print(train_x.shape)\n",
        "print(val_x.shape)\n",
        "print(test_x.shape)\n",
        "print(ood_x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZJ86_lV4vB-",
        "colab_type": "text"
      },
      "source": [
        "# Show Some Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oAYDQVFT5ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_random_images(img, labels, K, qualifier):\n",
        "  for k in range(K):\n",
        "    idx = np.random.randint(0, img.shape[0])\n",
        "    print(\"{} {}: {}\".format(qualifier, idx, class_dict[labels[idx]]))\n",
        "    plt.imshow(img[idx,:,:,:])\n",
        "    plt.show()\n",
        "\n",
        "show_random_images(train_x, train_y, 3, 'train')\n",
        "show_random_images(val_x, val_y, 3, 'validation')\n",
        "show_random_images(test_x, test_y, 3, 'test')\n",
        "show_random_images(ood_x, ood_y, 3, 'out of domain')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKKUf7BDbB-j",
        "colab_type": "text"
      },
      "source": [
        "# Make and Train Model\n",
        "\n",
        "Implement a convolutional neural network similar to the one in [1].\n",
        "We will simplify the architecture a bit, since we are dealing with Tiny Imagenet here, and since we would have trouble training the original model in Colab:\n",
        "\n",
        "1.   For the first convolutional layer, use a kernel size of 4 and stride 1, but still 96 filters.\n",
        "2.   For the *hidden* fully connected layers, use 2048 units, instead of 4096.\n",
        "\n",
        "Otherwise, use the same architecture as in [1].\n",
        "\n",
        "Some hints and remarks:\n",
        "\n",
        "*   Use 'same' padding for all convolutional and pooling layers.\n",
        "*   For the last layer, use *num_train_classes* (defined above) units.\n",
        "*   For all layers use *relu* activation functions, except for the last layer, where you should use the *softmax* activation.\n",
        "*   Apply dropout with dropout rate 0.5 before the two hidden fully connected layers.\n",
        "*   Train the model with the Adam optimizer, using a learning rate of 0.0001 and set *amsgrad=True*.\n",
        "*   Use early stopping [2] by calling model.fit(...) with argument *callbacks=[early_stopping_callback]*. The early_stopping_callback is already defined below. You'll need to provide the validation set as argument *validation_data* in model.fit(...).\n",
        "*   Train using *crossentropy* loss. You can use \n",
        "losses.sparse_categorical_crossentropy, since labels are not encoded in one-hot encoding.\n",
        "*   Use a *batch size* of 100.\n",
        "*   Train for maximal 100 epochs (early stopping will likely stop training much earlier).\n",
        "*   During training, measure *accuracy* and *top-5 accuracy*. You can use *sparse_top_k_categorical_accuracy*, since labels are not encoded in one-hot encoding.\n",
        "\n",
        "[2] https://en.wikipedia.org/wiki/Early_stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2HMxd2uqO-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model():\n",
        "\n",
        "  # keras model\n",
        "  model = Sequential()\n",
        "\n",
        "  # Make your model here\n",
        "  # ...\n",
        "\n",
        "  # ...\n",
        "\n",
        "  return model\n",
        "\n",
        "def make_model_and_train():\n",
        "\n",
        "  if not os.path.isdir(base_path):\n",
        "    raise AssertionError('No folder base_path. Please run cell \"Mount google drive\" above.')\n",
        "\n",
        "  model = make_model()\n",
        "  model.summary()\n",
        "\n",
        "  early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                          min_delta=0,\n",
        "                                                          patience=0,\n",
        "                                                          verbose=1, \n",
        "                                                          mode='auto')\n",
        "\n",
        "  # run training here\n",
        "  # ...\n",
        "\n",
        "  # ...\n",
        "\n",
        "  # save model to google drive\n",
        "  model.save(os.path.join(base_path, 'model.h5'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "#\n",
        "make_model_and_train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9Z-7_N7g3n",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Model\n",
        "\n",
        "Evaluate the crossentropy, classification accuracy and top-5 classification accuracy on the train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7qVHJK4CAQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(os.path.join(base_path, 'model.h5'))\n",
        "\n",
        "# evaluate model here\n",
        "# ...\n",
        "\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p8s1FxUN4nC",
        "colab_type": "text"
      },
      "source": [
        "# Image Retrieval\n",
        "\n",
        "We are now using the trained model for image retrieval on the out-of-domain dataset ood_x. \n",
        "We are considering, in turn, single images from *ood_x* as query image, and the remaining 4,999 images as retrieval database.\n",
        "The task of image retrieval (IR) is to find the *K* most similar images to the query image.\n",
        "In [1], similarity is defined as Euclidean distance between l2-normalised neural codes, where neural codes are simply the outputs of particular ConvNet layers.\n",
        "\n",
        "For each of the *3* layers which were considered in [1], perform the following steps:\n",
        "\n",
        "1.  Perform image retrieval for the first *10* images from *ood_x*. Retrieve the *K=5* most similar images for each query. Show the query image and the retrieved images next to each other, and mark the retrieved images which have the same class (stored in *ood_y*) as the query image. See Fig. 2 and 3 in [1] for examples (your results do not need to look precisely like in the paper. E.g., you can use *imshow* and *subplot*, and simply use *print* for the labels). \n",
        "\n",
        "2.   Compute and report the *mean average precision* (mAP), by computing  the *average precision* (AP) for each image in *ood_x*, and taking the mean AP over all 5,000 images.\n",
        "\n",
        "Hints:\n",
        "\n",
        "*   Make sure that the model is properly loaded, see previous cell.\n",
        "*   To obtain the neural codes, you can use the provided function eval_layer_batched(model, layer_name, ood_x, 100). It evaluates the layer with name *layer_name* for the whole ood_x. For example, if the layer contains 1024 units, this function will return a numpy array of size 5000x1024, with rows corresponding to images and columns to units.\n",
        "*   The AP is defined as follows.\n",
        "  *   Let *TP* be the number of *true positives*, that is, the number of retrieved images which have the *same* label as the query image.\n",
        "  *   Let *FP* be the number of *false positives*, that is, the number of retrieved images which have a *different* label than the query image.\n",
        "  *   Let *FN* be the number of *false negatives*, that is, the number of *non-retrieved* images, which have the *same* label as the query image.\n",
        "  *   The *precision* of an IR algorithm is defined as *precision* := TP / (TP + FP).   \n",
        "  *   The *recall* is defined as *recall* := TP / (TP + FN).\n",
        "  *   To better understand precision and recall, figure a haystack with some needles in it. Precision will be high if you carefully select very few objects, where you are sure that these are needles. But recall will be low then. Recall will be high if you just grab and return the whole haystack. But precision will be low then. Thus, precision and recall are (usually) opposed to each other and represent a trade-off.\n",
        "  * This trade-off can typically be governed by some hyper-parameter, in our case *K*, the number of retrieved images. For large *K*, we have large recall but low precision, for small *K* we have higher precision but low recall.\n",
        "  * The trade-off can be inspected by looking at the precision-recall curve. The AP is defined as area under the precision-recall curve.\n",
        "  *   Fortunately, an estimator of AP is already implemented for you in the function *average_precision*. It takes two arguments:\n",
        "     *  sorted_class_vals: list of **class values** of the 4,999 other images, sorted according to closeness to the query image (closest first, most distant last).\n",
        "     *  true_class: the class values of the query image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtMlvuNBgETQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()\n",
        "\n",
        "def get_layer_functor(model, layer_name):\n",
        "  inp = model.input \n",
        "  output =  model.get_layer(layer_name).output\n",
        "  return K.function([inp], [output])\n",
        "\n",
        "def eval_layer(x, layer_functor):\n",
        "  return layer_functor(x)[0]\n",
        "\n",
        "def eval_layer_batched(model, layer_name, x, batch_size):\n",
        "  layer_functor = get_layer_functor(model, layer_name)\n",
        "  idx = 0\n",
        "  ret_vals = None\n",
        "  while idx < x.shape[0]:\n",
        "    if idx + batch_size > x.shape[0]:\n",
        "      batch_x = x[idx:, ...]\n",
        "    else:\n",
        "      batch_x = x[idx:(idx+batch_size), ...]\n",
        "\n",
        "    batch_vals = eval_layer(batch_x, layer_functor)\n",
        "    if ret_vals is None:\n",
        "      ret_vals = batch_vals\n",
        "    else:\n",
        "      ret_vals = np.concatenate((ret_vals, batch_vals), 0)\n",
        "\n",
        "    idx += batch_size\n",
        "  return ret_vals\n",
        "\n",
        "def average_precision(sorted_class_vals, true_class):\n",
        "  ind = sorted_class_vals == true_class\n",
        "  num_positive = np.sum(ind)\n",
        "  cum_ind = np.cumsum(ind).astype(np.float32)\n",
        "  enum = np.array(range(1, len(ind)+1)).astype(np.float32)\n",
        "  return np.sum(cum_ind * ind / enum) / num_positive\n",
        "\n",
        "# perform image retrieval here\n",
        "# ...\n",
        "\n",
        "# ...\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}