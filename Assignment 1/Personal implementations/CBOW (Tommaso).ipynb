{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"CBOW (Tommaso).ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tatgD5IddeNP","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","from typing import List, Tuple, Union\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Layer, Flatten"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d87IVSL6deNj","colab_type":"text"},"source":["# CBOW\n","\n","Continuous Bag of Words implementation. I hope it makes sense."]},{"cell_type":"markdown","metadata":{"id":"IS5Hm19NdeNl","colab_type":"text"},"source":["## Preprocessing\n","\n","Read in alice text and then tokenize it."]},{"cell_type":"code","metadata":{"id":"37PyOHq2BkY4","colab_type":"code","colab":{}},"source":["# Removes sentences with fewer than 3 words\n","corpus = [sentence for sentence in raw_corpus if sentence.count(\" \") >= 2]\n","\n","# remove punctuation in text and fit tokenizer on entire corpus\n","tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n","tokenizer.fit_on_texts(corpus)\n","\n","# convert text to sequence of integer values\n","corpus = tokenizer.texts_to_sequences(corpus)\n","corpus_size = sum(len(s) for s in corpus) # total number of words in the corpus\n","voc_size = len(tokenizer.word_index) + 1 # total number of unique words in the corpus"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kt4k0mE7deOa","colab_type":"text"},"source":["## Dataset generation\n","\n","Our training task for CBOW is to guess a word based on the `window_size` words surrounding it either side.\n","Let's generate a dataset of `(window_words, word)` with fixed length window_words."]},{"cell_type":"code","metadata":{"id":"vsTLtYwpdeOe","colab_type":"code","colab":{}},"source":["def generate_dataset(\n","    corpus: List[List[int]],\n","    window_size: int\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    raw_x = []\n","    raw_y = []\n","    for line in corpus:\n","        for word_idx, word in enumerate(line):\n","            min_idx = max(0, word_idx - window_size)\n","            max_idx = min(len(line), word_idx + window_size)\n","            raw_x.append(\n","                line[min_idx : word_idx] + line[word_idx + 1 : max_idx + 1]\n","            )\n","            raw_y.append(word)\n","    x = pad_sequences(raw_x) # Pads all sequences to a fixed length\n","    y = np.array(raw_y)\n","    return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"igsr5VtwdeOt","colab_type":"code","colab":{}},"source":["window_size = 2\n","x, y = generate_dataset(corpus, window_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGSfAw_VdeO7","colab_type":"text"},"source":["## Model definition and training\n","\n","We then define the CBOW model and train it"]},{"cell_type":"markdown","metadata":{"id":"Dmen5YTndeO9","colab_type":"text"},"source":["We need a custom `AverageLayer` to average out the embeddings of the window words output from the embedding layer."]},{"cell_type":"code","metadata":{"id":"egQPzEyvdeO_","colab_type":"code","colab":{}},"source":["class AverageLayer(Layer):\n","    def __init__(self):\n","        super(AverageLayer, self).__init__()\n","    \n","    def call(self, inputs):\n","        return tf.math.reduce_mean(inputs, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AcfOUKqTdePO","colab_type":"text"},"source":["Then we can construct the model itself. We can vary the size of the embedding, optimizer and loss function"]},{"cell_type":"code","metadata":{"id":"t13s8iohdePP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"4b4cdba5-043b-46a2-942b-6c734dbb8703","executionInfo":{"status":"ok","timestamp":1589365294684,"user_tz":-120,"elapsed":1685,"user":{"displayName":"Tommaso Bonomo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFvz9TYkjc3qXXXhAwietnkHUX96TSaQl2pAP15Q=s64","userId":"12850357366043369607"}}},"source":["embedding_size = 100\n","cbow = Sequential([\n","    Embedding(voc_size, embedding_size, mask_zero=True, input_length=2 * window_size),\n","    AverageLayer(),\n","    Dense(voc_size, activation=\"softmax\", use_bias=False, kernel_regularizer=\"l2\")\n","])\n","cbow.compile(\n","    optimizer=\"adam\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"]\n",")\n","cbow.summary()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 4, 100)            255700    \n","_________________________________________________________________\n","average_layer_1 (AverageLaye (None, 100)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2557)              255700    \n","=================================================================\n","Total params: 511,400\n","Trainable params: 511,400\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ms0JIQl9dePZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"2da4d0e3-9636-4b1f-e865-d9dfb3427337","executionInfo":{"status":"ok","timestamp":1589365333365,"user_tz":-120,"elapsed":39715,"user":{"displayName":"Tommaso Bonomo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFvz9TYkjc3qXXXhAwietnkHUX96TSaQl2pAP15Q=s64","userId":"12850357366043369607"}}},"source":["early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n","_ = cbow.fit(x, y, batch_size=16, epochs=3) #, validation_split=0.1, callbacks=[early_stopping])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","1698/1698 [==============================] - 13s 8ms/step - loss: 7.2377 - accuracy: 0.0595\n","Epoch 2/3\n","1698/1698 [==============================] - 13s 8ms/step - loss: 6.7456 - accuracy: 0.0604\n","Epoch 3/3\n","1698/1698 [==============================] - 12s 7ms/step - loss: 6.5986 - accuracy: 0.0629\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gPUrh0PndePx","colab_type":"text"},"source":["## Evaluation of embeddings"]},{"cell_type":"markdown","metadata":{"id":"Se823szPdePz","colab_type":"text"},"source":["We first get the embedding layer from the model"]},{"cell_type":"code","metadata":{"id":"3iHXN9TFdeP3","colab_type":"code","colab":{}},"source":["embedding_layer = cbow.layers[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6cEABDLdeQA","colab_type":"text"},"source":["We then define a function to retrieve the embedding of a word. Can pass custom tokenizer or embedding_layer, but will otherwise use what we defined\n","above."]},{"cell_type":"code","metadata":{"id":"EEOhDuwhdeQC","colab_type":"code","colab":{}},"source":["def embed(\n","    word: str,\n","    tokenizer: Tokenizer = tokenizer,\n","    embedding_layer: Embedding = embedding_layer\n",") -> np.ndarray:\n","    word_index = tokenizer.texts_to_sequences([word])\n","    output_tensor = embedding_layer(np.array(word_index))\n","    return tf.reshape(output_tensor, (-1, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ihLnRCXgdeQP","colab_type":"text"},"source":["We also define a function that retrieves the closes word to a word vector"]},{"cell_type":"code","metadata":{"id":"Fb02NbHbdeQR","colab_type":"code","colab":{}},"source":["def cosine_similarity(\n","    v: Union[np.ndarray, tf.Tensor],\n","    w: Union[np.ndarray, tf.Tensor]) -> float:\n","    \"\"\"v and w should be 1D-vectors, for which to calculate the cosine similarity\"\"\"\n","    \n","    assert isinstance(v, tf.Tensor) or isinstance(v, np.ndarray), \"v must be tf.Tensor or np.ndarray\"\n","    assert isinstance(w, tf.Tensor) or isinstance(w, np.ndarray), \"w must be tf.Tensor or np.ndarray\"\n","    \n","    v = v.numpy() if isinstance(v, tf.Tensor) else v\n","    w = w.numpy() if isinstance(w, tf.Tensor) else w\n","    \n","    if v.ndim > 1:\n","        v = v.flatten()\n","    if w.ndim > 1:\n","        w = w.flatten()\n","\n","    return v @ w.T / (np.linalg.norm(v) * np.linalg.norm(w))\n","    \n","def closest_n_words(\n","    embedding: tf.Tensor,\n","    n: int = 1,\n","    tokenizer: Tokenizer = tokenizer,\n","    embedding_layer: Embedding = embedding_layer\n",") -> List[str]:    \n","    all_words = np.array(list(tokenizer.index_word.values()))\n","    all_embeddings = np.array([embed(word).numpy() for word in all_words]).squeeze()\n","    similarities = np.apply_along_axis(\n","        lambda row: cosine_similarity(row, embedding),\n","        1, all_embeddings\n","    )\n","    sorted_idx = np.argsort(similarities)\n","    \n","    return np.flip(all_words[sorted_idx[-n:]]).tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHWXu6VFdeQZ","colab_type":"text"},"source":["Ideally, we might think that $e_\\text{king} - e_\\text{man} \\approx e_\\text{queen} - e_\\text{woman}$. So if we try and find the closest word to \n","$e_\\text{king} - e_\\text{queen} + e_\\text{woman}$ it should be close to $e_\\text{man}$"]},{"cell_type":"code","metadata":{"id":"DEkH-2c6deQa","colab_type":"code","colab":{}},"source":["analogy = embed(\"king\") - embed(\"queen\") + embed(\"woman\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqf0EfwNdeQl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"97880cf5-222b-40fe-d287-a4d3191a1961","executionInfo":{"status":"ok","timestamp":1589365359912,"user_tz":-120,"elapsed":4907,"user":{"displayName":"Tommaso Bonomo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFvz9TYkjc3qXXXhAwietnkHUX96TSaQl2pAP15Q=s64","userId":"12850357366043369607"}}},"source":["closest_n_words(analogy, 10)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['woman',\n"," 'king',\n"," 'prisoner',\n"," 'english',\n"," 'jurymen',\n"," 'sea',\n"," 'temper',\n"," 'number',\n"," 'calling',\n"," 'box']"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"BixwmxOjdeQt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0a7fc944-1f42-4fec-d741-b4a792885129","executionInfo":{"status":"ok","timestamp":1589365363865,"user_tz":-120,"elapsed":1015,"user":{"displayName":"Tommaso Bonomo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiFvz9TYkjc3qXXXhAwietnkHUX96TSaQl2pAP15Q=s64","userId":"12850357366043369607"}}},"source":["cosine_similarity(analogy, embed(\"man\"))"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.94519687"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"lDzgOkGyjs_s","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}