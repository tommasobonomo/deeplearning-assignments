{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8zWrc_0qkin"
   },
   "source": [
    "# Assignment 3, Question 2\n",
    "\n",
    "<b>Group [fill in group number]</b>\n",
    "* <b> Student 1 </b> : FILL IN STUDENT NAME + STUDENT NUMBER\n",
    "* <b> Student 2 </b> : FILL IN STUDENT NAME + STUDENT NUMBER\n",
    "\n",
    "**Reading material**\n",
    "* [1] *Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, \"Show and Tell: A Neural Image Caption Generator\"*, CVPR, 2015. https://arxiv.org/abs/1411.4555\n",
    "\n",
    "**Task:**\n",
    "Implement and test the image caption generator proposed in [1], see further instructions below. \n",
    "Please insert your code between two consecutive occurrences of # ...\n",
    "\n",
    "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38FubjlHqmrv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Dropout, concatenate\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# training parameters\n",
    "embedding_dim = 512\n",
    "lstm_dim = 500\n",
    "lstm_dropout = 0.5\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv8_f0tWjzI6"
   },
   "source": [
    "# Mount Google Drive\n",
    "We will save the data and our model there, in the folder deeplearning2020_ass3_task1.\n",
    "**This requires about 600 MB**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnULuEVMiumR"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('drive'):\n",
    "  drive.mount('drive')\n",
    "else:\n",
    "  print('drive already mounted')\n",
    "\n",
    "base_path = os.path.join('drive', 'My Drive', 'deeplearning2020_ass3_task1')\n",
    "if not os.path.isdir(base_path):\n",
    "  os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRfuzsiYlwYG"
   },
   "source": [
    "# Download Data - Flickr8k\n",
    "\n",
    "<b><font color='red'>Please don't distribute the dataset</font></b> \n",
    "\n",
    "This is a preprocessed version of the Flickr8k dataset, with punctuation and special tokens removed. Furthermore, any word which occurs less than 5 times in the whole corpus has been removed. The images have been rescaled to 128x128 RGB.\n",
    "\n",
    "**images:** numpy array (8091,128,128,3), uint8, holding 8091 RGB images.\n",
    "\n",
    "**captions:** collection of 5 human-annotated captions for each image. Stored as a python list of length 8091. \n",
    "*   *captions[i]* is a list of length 5, for i=0..8090, holding the 5 annotations for the i'th image.\n",
    "*   *captions[i][j]* is a caption, represented as a list of strings, for i=0..8090, j=0..4. \n",
    "*   For example: *captions[42][3] = ['a', 'young', 'boy', 'wearing', 'a', 'red', 'coat', 'is', 'playing', 'in', 'a', 'long', 'tunnel']*.\n",
    "*   Thus, there are in total 8091 * 5 = 40455 captions.\n",
    "\n",
    "<b><font color='red'>Please don't distribute the dataset</font></b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtPd5qPUivTR"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('drive'):\n",
    "  raise AssertionError('Google drive seems to be unmounted -- please run cell above.')\n",
    "  \n",
    "flickr_file = os.path.join(base_path, 'Flickr8k_processed.pkl')\n",
    "\n",
    "if not os.path.isfile(flickr_file):\n",
    "  start_time = time.time()\n",
    "  if not os.path.isfile(flickr_file):\n",
    "    ! wget https://surfdrive.surf.nl/files/index.php/s/kOIDM5tQPzv6IID/download -O Flickr8k_processed.pkl\n",
    "    shutil.move('Flickr8k_processed.pkl', flickr_file)\n",
    "  print(\"Elapsed time: {} seconds.\".format(time.time()-start_time))\n",
    "else:\n",
    "  print('Found file {}'.format(flickr_file))\n",
    "\n",
    "images, captions = pickle.load(open(flickr_file, 'rb'))\n",
    "\n",
    "print('Data loaded.')\n",
    "print('images: {} {} {}'.format(images.shape, type(images), images.dtype))\n",
    "print('captions: {} {}'.format(len(captions), type(captions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_zbkOffvxj2"
   },
   "source": [
    "# Extract Image Representation\n",
    "\n",
    "* Use the 'Conv_1' layer from *MobileNetV2* to generate neural codes for each image in the array *images*. \n",
    "* Please generate a (8091,20480) numpy array in single precision (dtype=np.float32) holding the neural codes, where each row holds the code for the corresponding row in *images*. \n",
    "* Call the resulting array *image_codes*.\n",
    "* **Hint:** Process the images in batches (of e.g. 200), as the GPU won't be able to process all 8091 images in parallel.\n",
    "* **Hint:** MobileNetV2 requires images in floating point as inputs, with pixels rescaled to range [-1,1]. In order to save some RAM (and reduce troubles with Colab running out of resources), convert only the batches into single precision, and keep the *images* in their original format (uint8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5ltumKVv018"
   },
   "outputs": [],
   "source": [
    "def get_image_codes(images):      \n",
    "  convnet = MobileNetV2(input_shape=(128,128,3), \n",
    "                          include_top=False, \n",
    "                          weights='imagenet')\n",
    "  convnet.summary()\n",
    "\n",
    "  # ...\n",
    "  \n",
    "  # image_codes = ...\n",
    "  \n",
    "  # ...\n",
    "\n",
    "  return image_codes\n",
    "\n",
    "image_codes = get_image_codes(images)\n",
    "print(image_codes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYMQMuO4Ycti"
   },
   "source": [
    "# Analyze Captions\n",
    "\n",
    "* Find the maximal caption length in the *captions* and store it in a variable *max_caption_length*.\n",
    "* Construct a collection of all words (i.e. strings) occurring in the captions, and count their occurrences. \n",
    "Include the special word '_' (the *stop word*, signaling the end of the captions) in this collection.\n",
    "* Construct a dictionary *word_to_idx* which maps words to integers as follows:\n",
    "    *   '_' ->  0\n",
    "    *   most frequent word -> 1\n",
    "    *   second most frequent word -> 2\n",
    "    *   ...\n",
    "\n",
    "* Construct a dictionary *idx_to_word* which inverts the mapping *word_to_idx*.\n",
    "* Store the number of unique words, including '_', in a variable *num_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6WZIO3PYf9F"
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "# max_caption_length = \n",
    "# word_dict = \n",
    "# word_to_idx =\n",
    "# idx_to_word = \n",
    "# num_words = \n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jax1F-0rUyLW"
   },
   "source": [
    "# Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSs2wqKtU4j_"
   },
   "outputs": [],
   "source": [
    "val_images = images[0:1000, ...]\n",
    "val_codes = image_codes[0:1000, ...]\n",
    "val_captions = [captions[k] for k in range(1000)]\n",
    "\n",
    "test_images = images[1000:2000, ...]\n",
    "test_codes = image_codes[1000:2000, ...]\n",
    "test_captions = [captions[k] for k in range(1000, 2000)]\n",
    "\n",
    "train_images = images[2000:, ...]\n",
    "train_codes = image_codes[2000:, ...]\n",
    "train_captions = [captions[k] for k in range(2000, images.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XivuX4q5d4HE"
   },
   "source": [
    "# Convert Train and Validation Data into Matrix Format\n",
    "\n",
    "This encodes the captions to integer matrices using the mapping *word_to_idx*. \n",
    "It also duplicates the corresponding image codes.\n",
    "The result is two matrices {train, val}_codes and {train, val}_y, which hold image codes and integer encoded captions, whose rows correspond to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhvfZYVkd9OP"
   },
   "outputs": [],
   "source": [
    "def convert_data(codes, captions, max_caption_length, word_to_idx):\n",
    "  if codes.shape[0] != len(captions):\n",
    "    raise AssertionError(\"Different number of codes and captions.\")\n",
    "  \n",
    "  N = codes.shape[0]\n",
    "  duplicate_codes = None\n",
    "  labels = None\n",
    "  for k in range(5):\n",
    "    cur_labels = np.zeros((N, max_caption_length), dtype=np.uint32)\n",
    "    for l in range(N):\n",
    "      for count, w in enumerate(captions[l][k]):\n",
    "        cur_labels[l, count] = word_to_idx[w]\n",
    "\n",
    "    if duplicate_codes is None:\n",
    "      duplicate_codes = codes\n",
    "      labels = cur_labels\n",
    "    else:\n",
    "      duplicate_codes = np.concatenate((duplicate_codes, codes), 0)\n",
    "      labels = np.concatenate((labels, cur_labels), 0)\n",
    "  \n",
    "  return duplicate_codes, labels\n",
    "\n",
    "train_codes, train_y = convert_data(train_codes, train_captions, max_caption_length, word_to_idx)\n",
    "val_codes, val_y = convert_data(val_codes, val_captions, max_caption_length, word_to_idx)\n",
    "\n",
    "print(train_codes.shape)\n",
    "print(train_y.shape)\n",
    "print(val_codes.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-XgI8fjhNEI"
   },
   "source": [
    "# Show Random Images from Train and Validation Sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUmQKpiFhODP"
   },
   "outputs": [],
   "source": [
    "def show_random_image_and_captions(images, labels, idx_to_word):\n",
    "  n = images.shape[0]\n",
    "  idx = np.random.randint(0, n)\n",
    "\n",
    "  plt.imshow(images[idx % images.shape[0], ...])\n",
    "  plt.show()\n",
    "\n",
    "  encoded_caption = labels[idx, ...]\n",
    "  encoded_caption = [k for k in encoded_caption if k >= 0]\n",
    "  caption = [idx_to_word[i] for i in encoded_caption]\n",
    "  print(caption)\n",
    "  print('\\n\\n')\n",
    "\n",
    "show_random_image_and_captions(train_images, train_y, idx_to_word)\n",
    "show_random_image_and_captions(val_images, val_y, idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tguSD_Kzu-gS"
   },
   "source": [
    "# Make Model\n",
    "\n",
    "The model takes two inputs:\n",
    "\n",
    "*   *image_input*: placeholder for image codes.\n",
    "*   *caption_inputs*: placeholder for integer-encoded captions. \n",
    "\n",
    "You need to insert the following structure:\n",
    "\n",
    "*   Image embedding: *Dense* layer, mapping image codes to embeddings of length *embedding_dim*.\n",
    "*   Caption embedding: *Embedding* layer, mapping integers to embeddings of length *embedding_dim*.\n",
    "*   Concatenate Image embedding and Caption embeddings along the time axis. The image embedding should be at time t=0.\n",
    "*   LSTM with *lstm_dim* units, taking the concatenated embedding as input.\n",
    "*   Apply Dropout with rate 0.5 to the LSTM.\n",
    "*   Output layer: *Dense* layer, mapping the output of the LSTM to a categorical distribution (via *softmax*) of length *num_words*.\n",
    "\n",
    "**Hint:** The function K.expand_dims() might be useful here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQIVoQrNvBiw"
   },
   "outputs": [],
   "source": [
    "def make_model(code_length, max_caption_length, embedding_dim, num_words, lstm_dim, lstm_dropout):\n",
    "\n",
    "  # inputs\n",
    "  image_input = Input(shape=(code_length,))\n",
    "  caption_input = Input(shape=(max_caption_length - 1,))\n",
    "\n",
    "  # ...\n",
    "\n",
    "  # construct model here\n",
    "\n",
    "  # output = ...\n",
    "\n",
    "  # ...\n",
    "\n",
    "  return Model([image_input, caption_input], output)\n",
    "\n",
    "model = make_model(code_length=train_codes.shape[1], \n",
    "                   max_caption_length=max_caption_length, \n",
    "                   embedding_dim=embedding_dim, \n",
    "                   num_words=num_words, \n",
    "                   lstm_dim=lstm_dim, \n",
    "                   lstm_dropout=lstm_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VjbLX-lewCx"
   },
   "source": [
    "# Train Model\n",
    "\n",
    "* Use Adam with learning rate 0.001 and early stopping with patience 1. \n",
    "Provide the separate validation set for early stopping.\n",
    "* Use a batch size of 100.\n",
    "* Use a maximal number of epochs of 100 (early stopping will likely stop training much earlier).\n",
    "* Use crossentropy as loss function.\n",
    "* Report which data serves as input and which serves as output, and why.\n",
    "* **Hint:** Use the sparse version of crossentropy, in order to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IP9_k1Jey81"
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
    "                                        min_delta=0,\n",
    "                                        patience=1,\n",
    "                                        verbose=1, \n",
    "                                        mode='auto')\n",
    "\n",
    "# ...\n",
    "\n",
    "# ...\n",
    "\n",
    "model.save(os.path.join(base_path, 'model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYKO6aOc3b8Q"
   },
   "source": [
    "# Evaluate Model\n",
    "\n",
    "* Evaluate and report the final train and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChgJhlyB3eQR"
   },
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(base_path, 'model.h5'))\n",
    "model.summary()\n",
    "\n",
    "# ...\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utP4Fnf2_ug3"
   },
   "source": [
    "# Make Decoder\n",
    "\n",
    "* Make a greedy decoder model, which iteratively predicts the most likely word at each time step. The decoder is akin to the trained model above, but with a crucial difference: at time step t (t > 0), the LSTM takes the embedding of the word *predicted at time step t-1* as input. At time t=0, the LSTM takes the image embedding as input.\n",
    "* The decoder should return the predicted captions, encoded as integer matrix of shape (batch_size, max_caption_length).\n",
    "* Equip the decoder with the weights of the trained model. \n",
    "* **Hint:** You will need to pass on the internal state of the LSTM from time step to time step. To this end, use the argument *return_state=True* when creating the LSTM, and the *initial_state* argument when calling the LSTM. \n",
    "* **Hint:** Use the argument *weights* to pass the parameters of the trained model. This should contain the weights for image embedding, word embedding, LSTM, and output layer. Use the methods *get_weights()* and *set_weights()* to this end.\n",
    "* **Hint:** The functions *K.expand_dims()*, *K.argmax()*, and *K.stack()* might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-aC5Jie_xm0"
   },
   "outputs": [],
   "source": [
    "def make_decoder(code_length, max_caption_length, embedding_dim, num_words, lstm_dim, lstm_dropout, weights):\n",
    "  \n",
    "  # input\n",
    "  image_input = Input(shape=(code_length,))\n",
    "  \n",
    "  # ...\n",
    "\n",
    "  # ...\n",
    "\n",
    "  return model\n",
    "\n",
    "# Get the weights from trained model, and put them in a list 'weights'.\n",
    "# ...\n",
    "\n",
    "# weights = [ ... ]\n",
    "\n",
    "# ...\n",
    "\n",
    "decoder = make_decoder(code_length=train_codes.shape[1], \n",
    "                       max_caption_length=max_caption_length,\n",
    "                       embedding_dim=embedding_dim,\n",
    "                       num_words=num_words, \n",
    "                       lstm_dim=lstm_dim, \n",
    "                       lstm_dropout=lstm_dropout,\n",
    "                       weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kpuJt21koiQc"
   },
   "source": [
    "# Predict Test Captions\n",
    "\n",
    "*   Use the decoder to predict the test captions.\n",
    "*   Decode them to text using the mapping *idx_to_word*.\n",
    "*   Show 10 random test images and their predicted captions. Categorize them like in Figure 5 in the paper.\n",
    "*   Report the 1-gram, 2-gram, 3-gram, and 4-gram BLEU scores of the test predictions. **Hint:** You can use the *nltk* package for this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whGa9ngJ77ZZ"
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2IMM10_Assignment_3_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
