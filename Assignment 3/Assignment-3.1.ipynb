{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5BoXrK_u3u_"
   },
   "source": [
    "# Assignment 3.1. Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sr9zwNA-u3vH"
   },
   "source": [
    "# Task: Aspect-level Sentiment Classification(10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y506kXSAu3vP"
   },
   "source": [
    "Reading material:\n",
    "- [1] R. He, WS. Lee & D. Dahlmeier. Exploiting document knowledge for aspect-level sentiment classification. 2018. https://arxiv.org/abs/1806.04346.\n",
    "\n",
    "\n",
    "Build an attention-based aspect-level sentiment classification model with biLSTM. Your model shall include:\n",
    "\n",
    "- BiLSTM network that learns sentence representation from input sequences.\n",
    "- Attention network that assigns attention score over a sequence of biLSTM hidden states based on aspect terms representation.\n",
    "- Fully connected network that predicts sentiment label, given the representation weighted by the attention score.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- You shall train your model bsaed on transferring learning. That is, you need first train your model on documnet-level examples. Then the learned weights will be used to initialize aspect-level model and fine tune it on aspect-level examples.\n",
    "- You shall use the alignment score function in attention network as following expression:$$f_{score}(h,t)=tanh(h^TW_a t)$$\n",
    "- You shall evaluate the trained model on the provided test set and show the accuracy on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPDMoaQNNAC5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymEccUbRNEGd"
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9TH6Um2s-7d"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbWy_h2KtNgR"
   },
   "outputs": [],
   "source": [
    "def read_pickle(data_path, file_name):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'rb')\n",
    "    read_file = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return read_file\n",
    "\n",
    "def save_pickle(data_path, file_name, data):\n",
    "\n",
    "    f = open(os.path.join(data_path, file_name), 'wb')\n",
    "    cPickle.dump(data, f)\n",
    "    print(\" file saved to: %s\"%(os.path.join(data_path, file_name)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuAMxdYBkv_x"
   },
   "outputs": [],
   "source": [
    "aspect_path = 'data/aspect-level' \n",
    "\n",
    "\n",
    "vocab = read_pickle(aspect_path, 'all_vocab.pkl')\n",
    "\n",
    "train_x = read_pickle(aspect_path, 'train_x.pkl')\n",
    "train_y = read_pickle(aspect_path, 'train_y.pkl')\n",
    "dev_x = read_pickle(aspect_path, 'dev_x.pkl')\n",
    "dev_y = read_pickle(aspect_path, 'dev_y.pkl')\n",
    "test_x = read_pickle(aspect_path, 'test_x.pkl')\n",
    "test_y = read_pickle(aspect_path, 'test_y.pkl')\n",
    "\n",
    "train_aspect = read_pickle(aspect_path, 'train_aspect.pkl')\n",
    "dev_aspect = read_pickle(aspect_path, 'dev_aspect.pkl')\n",
    "test_aspect = read_pickle(aspect_path, 'test_aspect.pkl')\n",
    "\n",
    "\n",
    "pretrain_data = read_pickle(aspect_path, 'pretrain_data.pkl')\n",
    "pretrain_label = read_pickle(aspect_path, 'pretrain_label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWVgGO8RlVIJ"
   },
   "outputs": [],
   "source": [
    "class Dataiterator_doc():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, seq_length=32, decoder_dim=300, batch_size=32):      \n",
    "        self.X = X \n",
    "        self.y = y \n",
    "        self.num_data = len(X) # total number of examples\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X = self.X[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y = self.y[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        return batch_X, batch_y\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X, self.y\n",
    "class Dataiterator_aspect():\n",
    "    '''\n",
    "      1) Iteration over minibatches using next(); call reset() between epochs to randomly shuffle the data\n",
    "      2) Access to the entire dataset using all()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, aspect_data, seq_length=32, decoder_dim=300, batch_size=32):\n",
    "        \n",
    "        len_aspect_data = len(aspect_data[0])\n",
    "        #self.len_doc_data = len(doc_data[0])\n",
    "        \n",
    "        self.X_aspect = aspect_data[0] \n",
    "        self.y_aspect = aspect_data[1]\n",
    "        self.aspect_terms = aspect_data[2]  \n",
    "        self.num_data = len_aspect_data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.reset() # initial: shuffling examples and set index to 0\n",
    "    \n",
    "    def __iter__(self): # iterates data\n",
    "        return self\n",
    "\n",
    "\n",
    "    def reset(self): # initials\n",
    "        self.idx = 0\n",
    "        self.order = np.random.permutation(self.num_data) # shuffling examples by providing randomized ids \n",
    "        \n",
    "    def __next__(self): # return model inputs - outputs per batch\n",
    "        \n",
    "        X_ids = [] # hold ids per batch \n",
    "        while len(X_ids) < self.batch_size:\n",
    "            X_id = self.order[self.idx] # copy random id from initial shuffling\n",
    "            X_ids.append(X_id)\n",
    "            self.idx += 1 # \n",
    "            if self.idx >= self.num_data: # exception if all examples of data have been seen (iterated)\n",
    "                self.reset()\n",
    "                raise StopIteration()\n",
    "                \n",
    "        batch_X_aspect = self.X_aspect[np.array(X_ids)] # X values (encoder input) per batch\n",
    "        batch_y_aspect = self.y_aspect[np.array(X_ids)] # y_in values (decoder input) per batch\n",
    "        batch_aspect_terms = self.aspect_terms[np.array(X_ids)]\n",
    "        \n",
    "        return batch_X_aspect, batch_y_aspect, batch_aspect_terms\n",
    "\n",
    "          \n",
    "    def all(self): # return all data examples\n",
    "        return self.X_aspect, self.y_aspect, self.aspect_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38IEEx0du3vW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Lambda, Dropout, LSTM,Bidirectional, Flatten\n",
    "from keras.layers import Reshape, Activation, RepeatVector, concatenate, Concatenate, Dot, Multiply\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cduHSpsSnVue"
   },
   "outputs": [],
   "source": [
    "overal_maxlen = 82\n",
    "overal_maxlen_aspect = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhTQke3HnvHN"
   },
   "source": [
    "# Define Attention Network Layer\n",
    "- Define class for Attention Layer\n",
    "- You need to finish the code for calculating the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnDX-po3_50B"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,  **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Content Attention mechanism.\n",
    "        Supports Masking.\n",
    "        \"\"\"\n",
    "       \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert type(input_shape) == list\n",
    "       \n",
    "        self.steps = input_shape[0][1]\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[0][-1], input_shape[1][-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input_tensor, mask=None):\n",
    "        assert type(input_tensor) == list\n",
    "        assert type(mask) == list\n",
    "        return None\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        x = input_tensor[0]\n",
    "        aspect = input_tensor[1]\n",
    "        mask = mask[0]\n",
    "        ###YOUR CODE HERE###\n",
    "        \n",
    "        ### Must still do mask\n",
    "        # We must swap the axes of the result of x @ W @ a.T, so we have a tensor [82, None, None].\n",
    "        # We can then extract the diagonal vector for each vector in the first dimension, so that\n",
    "        # in the batch we pick the attention of x related to it's aspect terms.\n",
    "        # We finally transpose so to have shape [None, 82]\n",
    "        beta = tf.transpose(\n",
    "            tf.linalg.diag_part(\n",
    "                tf.transpose(\n",
    "                    tf.tanh(x @ self.W @ tf.transpose(aspect)),\n",
    "                    [1, 0, 2]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        alpha = tf.exp(beta) / tf.reduce_sum(tf.exp(beta), axis=1, keepdims=True)\n",
    "        \n",
    "        return alpha\n",
    "\n",
    "   \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5PSPx85EiVn"
   },
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "  \n",
    "    def __init__(self, mask_zero=True, **kwargs):\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x,mask=None):\n",
    "        if self.mask_zero:           \n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "            return K.sum(x, axis=1) / (K.sum(mask, axis=1) + K.epsilon())\n",
    "        else:\n",
    "            return K.mean(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def compute_mask(self, x, mask):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GsZtx2PbEqoh"
   },
   "source": [
    "# Establish computation Grah for model\n",
    "- Input tensors\n",
    "- Shared WordEmbedding layer \n",
    "- Attention network layer  \n",
    "- Shared BiLSTM layer\n",
    "- Shared fully connected layer(prediction layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe55OCNZEmYY"
   },
   "outputs": [],
   "source": [
    "dropout = 0.5     \n",
    "recurrent_dropout = 0.1\n",
    "vocab_size = len(vocab)\n",
    "num_outputs = 3 # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BpzACdBp3xG"
   },
   "source": [
    "## Input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOi3CcOxE1MG"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE ##### Inputs #####\n",
    "aspect_inputs = Input(shape=(overal_maxlen_aspect,), dtype=\"int32\", name=\"aspect\")\n",
    "sentence_inputs = Input(shape=(overal_maxlen,), dtype=\"int32\", name=\"sentence\")\n",
    "pretrain_inputs = Input(shape=(None,), dtype=\"int32\", name=\"pretrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vQ0z8KmrL3_"
   },
   "source": [
    "## Shared WordEmbedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFEhEt9EE4Sn"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE### represent aspect as averaged word embedding ###\n",
    "emb_layer = Embedding(vocab_size, 300, mask_zero=True)\n",
    "aspect_embedding = emb_layer(aspect_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gGd94OGE-Gr"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE ### sentence representation from embedding ###\n",
    "sentence_embedding = emb_layer(sentence_inputs)\n",
    "pretrain_embedding = emb_layer(pretrain_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmcnAQufrc7o"
   },
   "source": [
    "## Shared BiLSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bm_yCjX_F7ml"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE ### sentence representation from embedding ###\n",
    "bilstm = Bidirectional(LSTM(500, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True))\n",
    "sentence_bilstm = bilstm(sentence_embedding)\n",
    "pretrain_bilstm = bilstm(pretrain_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99ZNrbkmrllN"
   },
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HO5Pj6QANz7U"
   },
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "average_aspect_embedding = Average()(aspect_embedding)\n",
    "attention_weights = Attention()([sentence_bilstm, average_aspect_embedding])\n",
    "attention_contex = Dot(axes=1)([attention_weights, sentence_bilstm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-LY6jF8r3mO"
   },
   "source": [
    "## Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLQCpLtHOBBH"
   },
   "outputs": [],
   "source": [
    "densed_output_aspect = Dense(3, activation=\"softmax\")(attention_contex)\n",
    "last_pretrained_bilstm = Lambda(lambda x: x[:, -1])(pretrain_bilstm)\n",
    "densed_output_pretrain = Dense(3, activation=\"softmax\")(last_pretrained_bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XLv1t9Ou3vx"
   },
   "source": [
    "# Build Models for document-level and aspect-level data\n",
    "- The two models shared the embedding, BiLSTM, Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX8qKfNWu3v0"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "model1 = Model(inputs=[pretrain_inputs], outputs=[densed_output_pretrain])\n",
    "model2 = Model(inputs=[sentence_inputs, aspect_inputs], outputs=[densed_output_aspect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FWqofwqBsgsn"
   },
   "source": [
    "# Train Model\n",
    "- First Train model on document-level data.\n",
    "- Then Train  model on aspect-level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSLYsZm7yPwi"
   },
   "source": [
    "## Train on document-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHe86c6Yu3wG"
   },
   "outputs": [],
   "source": [
    "\n",
    "import keras.optimizers as opt\n",
    "optimizer=opt.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, clipnorm=10, clipvalue=0)\n",
    "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "batch_size = 128\n",
    "train_steps_epoch = len(pretrain_data)/batch_size\n",
    "batch_train_iter_doc = Dataiterator_doc(pretrain_data, pretrain_label, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfEvhdbhFlbR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "235/234 [==============================] - 337s 1s/step - loss: 1.0511 - categorical_accuracy: 0.4625\n",
      "Epoch 2/20\n",
      "235/234 [==============================] - 334s 1s/step - loss: 0.9519 - categorical_accuracy: 0.5396\n",
      "Epoch 3/20\n",
      "235/234 [==============================] - 336s 1s/step - loss: 0.8928 - categorical_accuracy: 0.5799\n",
      "Epoch 4/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.8477 - categorical_accuracy: 0.6028\n",
      "Epoch 5/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.7947 - categorical_accuracy: 0.6418\n",
      "Epoch 6/20\n",
      "235/234 [==============================] - 337s 1s/step - loss: 0.7952 - categorical_accuracy: 0.6370\n",
      "Epoch 7/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.7798 - categorical_accuracy: 0.6499\n",
      "Epoch 8/20\n",
      "235/234 [==============================] - 334s 1s/step - loss: 0.7532 - categorical_accuracy: 0.6620\n",
      "Epoch 9/20\n",
      "235/234 [==============================] - 335s 1s/step - loss: 0.7013 - categorical_accuracy: 0.6939\n",
      "Epoch 10/20\n",
      "235/234 [==============================] - 332s 1s/step - loss: 0.6978 - categorical_accuracy: 0.6928\n",
      "Epoch 11/20\n",
      "235/234 [==============================] - 335s 1s/step - loss: 0.6922 - categorical_accuracy: 0.6969\n",
      "Epoch 12/20\n",
      "235/234 [==============================] - 342s 1s/step - loss: 0.6905 - categorical_accuracy: 0.6959\n",
      "Epoch 13/20\n",
      "235/234 [==============================] - 335s 1s/step - loss: 0.6224 - categorical_accuracy: 0.7364\n",
      "Epoch 14/20\n",
      "235/234 [==============================] - 332s 1s/step - loss: 0.6457 - categorical_accuracy: 0.7238\n",
      "Epoch 15/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.6258 - categorical_accuracy: 0.7316\n",
      "Epoch 16/20\n",
      "235/234 [==============================] - 331s 1s/step - loss: 0.6285 - categorical_accuracy: 0.7299\n",
      "Epoch 17/20\n",
      "235/234 [==============================] - 335s 1s/step - loss: 0.5575 - categorical_accuracy: 0.7693\n",
      "Epoch 18/20\n",
      "235/234 [==============================] - 331s 1s/step - loss: 0.5906 - categorical_accuracy: 0.7487\n",
      "Epoch 19/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.5759 - categorical_accuracy: 0.7570\n",
      "Epoch 20/20\n",
      "235/234 [==============================] - 333s 1s/step - loss: 0.5789 - categorical_accuracy: 0.7548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff8a841d4e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###YOUR CODE HERE###\n",
    "def train_generator_pretrain(model, batch_train_iter, train_steps_epoch, epochs):\n",
    "    def train_gen():\n",
    "        while True:\n",
    "            train_batches = [[X, y] for X, y in batch_train_iter]\n",
    "            for train_batch in train_batches:\n",
    "                yield train_batch\n",
    "                \n",
    "    history = model.fit(\n",
    "        train_gen(),\n",
    "        steps_per_epoch=train_steps_epoch,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    return history\n",
    "train_generator_pretrain(model1, batch_train_iter_doc, train_steps_epoch, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-SYaHNMyXWX"
   },
   "source": [
    "## Train on aspect-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCLGP6HTFsKL"
   },
   "outputs": [],
   "source": [
    "train_steps_epoch = len(train_x)/batch_size\n",
    "batch_train_iter_aspect = Dataiterator_aspect([train_x, train_y, train_aspect], batch_size)\n",
    "val_steps_epoch = len(dev_x)/batch_size\n",
    "batch_val_iter_aspect = Dataiterator_aspect([dev_x, dev_y, dev_aspect], batch_size)\n",
    "\n",
    "import keras.optimizers as opt\n",
    "optimizer = opt.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clipnorm=10, clipvalue=0)\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvT2GqG0LONz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/14 [===============================] - 8s 514ms/step - loss: 1.0637 - categorical_accuracy: 0.5521 - val_loss: 1.0099 - val_categorical_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "15/14 [===============================] - 6s 421ms/step - loss: 1.0237 - categorical_accuracy: 0.5667 - val_loss: 0.9999 - val_categorical_accuracy: 0.5547\n",
      "Epoch 3/20\n",
      "15/14 [===============================] - 6s 417ms/step - loss: 0.9371 - categorical_accuracy: 0.6375 - val_loss: 1.0344 - val_categorical_accuracy: 0.5859\n",
      "Epoch 4/20\n",
      "15/14 [===============================] - 6s 425ms/step - loss: 0.9119 - categorical_accuracy: 0.6021 - val_loss: 0.9367 - val_categorical_accuracy: 0.5859\n",
      "Epoch 5/20\n",
      "15/14 [===============================] - 7s 435ms/step - loss: 0.8758 - categorical_accuracy: 0.6187 - val_loss: 1.0599 - val_categorical_accuracy: 0.6250\n",
      "Epoch 6/20\n",
      "15/14 [===============================] - 6s 424ms/step - loss: 0.8746 - categorical_accuracy: 0.6187 - val_loss: 0.7769 - val_categorical_accuracy: 0.6875\n",
      "Epoch 7/20\n",
      "15/14 [===============================] - 6s 424ms/step - loss: 0.8525 - categorical_accuracy: 0.6229 - val_loss: 0.9478 - val_categorical_accuracy: 0.6406\n",
      "Epoch 8/20\n",
      "15/14 [===============================] - 6s 426ms/step - loss: 0.8390 - categorical_accuracy: 0.6313 - val_loss: 0.8435 - val_categorical_accuracy: 0.6328\n",
      "Epoch 9/20\n",
      "15/14 [===============================] - 6s 424ms/step - loss: 0.8235 - categorical_accuracy: 0.6354 - val_loss: 0.9840 - val_categorical_accuracy: 0.6641\n",
      "Epoch 10/20\n",
      "15/14 [===============================] - 6s 423ms/step - loss: 0.8393 - categorical_accuracy: 0.6375 - val_loss: 0.8105 - val_categorical_accuracy: 0.6562\n",
      "Epoch 11/20\n",
      "15/14 [===============================] - 6s 415ms/step - loss: 0.7821 - categorical_accuracy: 0.6479 - val_loss: 0.8865 - val_categorical_accuracy: 0.6797\n",
      "Epoch 12/20\n",
      "15/14 [===============================] - 6s 431ms/step - loss: 0.7855 - categorical_accuracy: 0.6313 - val_loss: 1.0694 - val_categorical_accuracy: 0.6562\n",
      "Epoch 13/20\n",
      "15/14 [===============================] - 6s 410ms/step - loss: 0.7907 - categorical_accuracy: 0.6687 - val_loss: 0.8922 - val_categorical_accuracy: 0.6328\n",
      "Epoch 14/20\n",
      "15/14 [===============================] - 7s 437ms/step - loss: 0.7487 - categorical_accuracy: 0.6979 - val_loss: 1.0054 - val_categorical_accuracy: 0.6797\n",
      "Epoch 15/20\n",
      "15/14 [===============================] - 6s 430ms/step - loss: 0.7783 - categorical_accuracy: 0.6500 - val_loss: 0.7003 - val_categorical_accuracy: 0.6641\n",
      "Epoch 16/20\n",
      "15/14 [===============================] - 6s 428ms/step - loss: 0.7693 - categorical_accuracy: 0.6625 - val_loss: 0.8382 - val_categorical_accuracy: 0.7109\n",
      "Epoch 17/20\n",
      "15/14 [===============================] - 6s 411ms/step - loss: 0.7255 - categorical_accuracy: 0.7063 - val_loss: 0.8483 - val_categorical_accuracy: 0.6484\n",
      "Epoch 18/20\n",
      "15/14 [===============================] - 6s 415ms/step - loss: 0.7382 - categorical_accuracy: 0.6896 - val_loss: 1.0756 - val_categorical_accuracy: 0.6016\n",
      "Epoch 19/20\n",
      "15/14 [===============================] - 6s 416ms/step - loss: 0.7525 - categorical_accuracy: 0.6854 - val_loss: 0.8178 - val_categorical_accuracy: 0.6484\n",
      "Epoch 20/20\n",
      "15/14 [===============================] - 6s 421ms/step - loss: 0.7035 - categorical_accuracy: 0.7000 - val_loss: 0.8521 - val_categorical_accuracy: 0.6562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff8a83e2a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_generator(\n",
    "    model: tf.keras.Model,\n",
    "    batch_train_iter: Dataiterator_aspect,\n",
    "    batch_val_iter: Dataiterator_aspect,\n",
    "    train_steps_epoch: float,\n",
    "    val_steps_epoch: float,\n",
    "    epochs: int\n",
    ") -> tf.keras.callbacks.History:\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)]\n",
    "    \n",
    "    def train_gen():\n",
    "        while True:\n",
    "            for batch_x_aspect, batch_y_aspect, batch_aspect_terms in batch_train_iter:\n",
    "                yield [[batch_x_aspect, batch_aspect_terms], batch_y_aspect]\n",
    "    \n",
    "    def val_gen():\n",
    "        while True:\n",
    "            for val_batch_x, val_batch_y, val_batch_terms in batch_val_iter:\n",
    "                yield [[val_batch_x, val_batch_terms], val_batch_y]\n",
    "    \n",
    "    return model.fit(\n",
    "        train_gen(),\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen(),\n",
    "        steps_per_epoch=train_steps_epoch,\n",
    "        validation_steps=val_steps_epoch\n",
    "    )\n",
    "train_generator(model2, batch_train_iter_aspect, batch_val_iter_aspect, train_steps_epoch, val_steps_epoch, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVPNUQcuyAU3"
   },
   "source": [
    "## Evaluating on test set\n",
    "- show the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_JQwuUHMisH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638/638 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8233680121577271, 0.6112852692604065]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##YOUR CODE HERE\n",
    "model2.evaluate(x=[test_x, test_aspect], y=test_y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-3.1.2.Aspect-Level-Sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
