{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practical_6.2_convolutional_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeLLU7zA5xgN"
      },
      "source": [
        "# Practical 6.2: Convolutional autoencoder architectures\n",
        "In this practical we will look at how to design a convolutional autoencoder architecture. For the encoder, we can use similar architectures as for common classification/regression networks (e.g. VGG-style). Typically, the decoder is then designed to \"reverse\" the structure of the encoder. For fully connected layers this is straightforward, but for convolutional or pooling layers this leaves some options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSYbR0LJFWrk",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BruKfi2NCZM2",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B_ghJwoMAOq9"
      },
      "source": [
        "## Data (MNIST)\n",
        "We'll use MNIST as a simple example. Images are 28x28 pixels, but for convolutional autoencoders with strides or pooling, it is very convenient if the image size is a multiple of the stride/pool size. Typically, we use 2x2 strides/pooling, so we rescale the canvas size of MNIST from 28x28 to 32x32 (since 32=2^5), by padding zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xk9zHRsyAOMI",
        "colab": {}
      },
      "source": [
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# normalise\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# change shape: (n, 28, 28) -> (n, 28, 28, 1)\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "\n",
        "# pad zeros to obtain canvas size of 32x32, i.e. shape is now (n, 32, 32, 1)\n",
        "x_train = np.pad(x_train, pad_width=((0, 0), (2, 2), (2, 2), (0, 0)), mode=\"constant\", constant_values=0)\n",
        "x_test = np.pad(x_test, pad_width=((0, 0), (2, 2), (2, 2), (0, 0)), mode=\"constant\", constant_values=0)\n",
        "\n",
        "# get number of training samples, height, width, and depth values\n",
        "n_train, height, width, depth = x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSMvUpCcBJh-"
      },
      "source": [
        "## VGG-like encoder\n",
        "Let's make a simple VGG-like encoder with 2 convolutional blocks, each consisting of a single convolution and maxpooling, followed by two dense layers. We'll also include batch normalization.\n",
        "\n",
        "Let's write the input and output shape of each layer in the comments, to get a better idea of the structure of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qWGPctQv5oKv",
        "colab": {}
      },
      "source": [
        "# input\n",
        "x_in = layers.Input(shape=(height, width, depth), name=\"encoder_input\")\n",
        "\n",
        "# conv block 1\n",
        "h = layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\",\n",
        "                  activation='relu')(x_in)                    # (32, 32, 1)  -> (32, 32, 64)\n",
        "h = layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\")(h)  # (32, 32, 64) -> (16, 16, 64)\n",
        "h = layers.BatchNormalization()(h)                            # no change\n",
        "\n",
        "# conv block 2\n",
        "h = layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\",\n",
        "                  activation='relu')(h)                       # (16, 16, 64)  -> (16, 16, 128)\n",
        "h = layers.MaxPooling2D(pool_size=(2, 2), padding=\"same\")(h)  # (16, 16, 128) -> (8, 8, 128)\n",
        "h = layers.BatchNormalization()(h)                            # no change\n",
        "\n",
        "# dense layers\n",
        "h = layers.Flatten()(h)                                       # (8, 8, 128) -> (8*8*128,)\n",
        "h = layers.Dense(256, activation=\"relu\")(h)                   # (8*8*128,)  -> (256,)\n",
        "h = layers.BatchNormalization()(h)                            # no change\n",
        "h = layers.Dense(128, activation=\"relu\")(h)                   # (256,) -> (128,)\n",
        "h = layers.BatchNormalization()(h)                            # no change\n",
        "encoded = layers.Dense(32, name=\"encoded\")(h)                 # (128,) -> (32,) (encoding dimension)\n",
        "# NOTE: no activation for the last layer, encoded representations are vectors of real numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nt_45hfWGfjm"
      },
      "source": [
        "## Decoder architecture: reversing the structure\n",
        "For our decoder architecture, it seems natural to reverse the structure of the encoder, layer by layer. We could try some weight-tying between encoder and decoder, but typically this is not done; the encoder and decoder each have their own weights that are trained independently.\n",
        "\n",
        "Reversing the structure of a fully connected layer is simple; use the input size of the encoder layer as the number of output units for the decoder layer.\n",
        "\n",
        "To reverse convolutional layers (in particular when they are strided), we could use a transposed convolution (often mistakenly called \"deconvolution\") to reverse its structure. However, this is prone to creating unwanted artifacts, or the checkerboard effect (see https://distill.pub/2016/deconv-checkerboard/ for an excellent explanation). Therefore, we prefer to not use strided convolutions in the encoder, and we use MaxPooling2D to downsample the size of the activation maps. To reverse this downsampling, we can use the UpSampling2D layer in Keras (which is essentially nearest-neighbour interpolation as described and recommended in https://distill.pub/2016/deconv-checkerboard/). To \"reverse\" a non-strided convolution, we can then simply use another normal convolution, where we use the number of input activation maps (filters) of the encoder conv-layer as the number of output activation maps (filters) for the decoder conv-layer.\n",
        "\n",
        "Now let's reverse the encoder architecture, making sure that we have the same change in shape for the hidden units, but reversed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Vmyu35SR9M",
        "colab_type": "text"
      },
      "source": [
        "## ASSIGNMENT:\n",
        "Design the decoder architecture, by reversing the encoder architecture. The reversed order of the shapes is already given as a guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RehLD1URrsnS",
        "colab": {}
      },
      "source": [
        "#### INSERT CODE HERE ####\n",
        "#    reverse dense layers:\n",
        "# (32,) -> (128,) (encoding dimension)\n",
        "# no change\n",
        "# (128, -> (256,)\n",
        "# no change\n",
        "# (256,)     -> (8*8*128,)\n",
        "# (8*8*128,) -> (8, 8, 128)\n",
        "\n",
        "#    reverse conv block 2:\n",
        "# no change\n",
        "# (8, 8, 128  ) -> (16, 16, 128)\n",
        "# (16, 16, 128) -> (16, 16, 64)\n",
        "\n",
        "#    reverse conv block 1:\n",
        "# no change\n",
        "# (16, 16, 64) -> (32, 32, 64)\n",
        "# (32, 32, 64) -> (32, 32, 1)\n",
        "# HINT: think about the activation for the last layer, which should represent\n",
        "#       pixel values between 0 and 1\n",
        "#### UNTIL HERE ####"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_Gfx42fuHPy"
      },
      "source": [
        "## Compile, train, and test the model\n",
        "Let's setup and compile the autoencoder, and do a quick test. We use some loss function to compare the original data with its reconstruction, so we use the training set both as input and target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O34SRp8-uGfC",
        "colab": {}
      },
      "source": [
        "autoencoder = Model(x_in, x_out, name=\"autoencoder\")\n",
        "autoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=20,\n",
        "                batch_size=100,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mgT1vEyD24ay"
      },
      "source": [
        "Reconstruct and visualise some images from the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9y7PSeOQz_Iy",
        "colab": {}
      },
      "source": [
        "n = 10  # number of images to plot\n",
        "indices = np.random.choice(len(x_test), size=n, replace=False)\n",
        "test_imgs = x_test[indices]\n",
        "reconstr_imgs = autoencoder.predict(test_imgs)\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(test_imgs[i].reshape(32, 32))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(reconstr_imgs[i].reshape(32, 32))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}