{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram\n",
    "\n",
    "Implementation of the Skipgram algorithm for word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\", \"r\") as f:\n",
    "    raw_corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + \"'\")\n",
    "tokenizer.fit_on_texts(raw_corpus)\n",
    "corpus = sum((tokenizer.texts_to_sequences(raw_corpus)), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = max(corpus) + 1\n",
    "corpus_size = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27330, 2568)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size, voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "\n",
    "Skipgram is trained on the following task: given a `window_size`, samples randomly a `word_range` between `1` and `window_size` and tries to guess each word `word_range` around the current word.\n",
    "So we need to construct a dataset of pairs `(word, target)` for each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "    corpus: List[int],\n",
    "    window_size: int,\n",
    "    seed: int = 4322\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    rand_generator = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    x: List[int] = []\n",
    "    y: List[int] = []    \n",
    "    for idx, word in enumerate(corpus):\n",
    "        word_range = rand_generator.integers(1, window_size, endpoint=True)\n",
    "        \n",
    "        surrounding_words = corpus[idx - word_range : idx] + corpus[idx + 1 : idx + word_range + 1]\n",
    "        x += surrounding_words\n",
    "        y += [word] * len(surrounding_words)\n",
    "    \n",
    "    np_x = np.array(x)\n",
    "    np_y = np.array(y)\n",
    "    \n",
    "    return np_x, np_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "x, y = generate_dataset(corpus, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram implementation\n",
    "\n",
    "Now we implement the skipgram model, should have quite a simple architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 100)            256800    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2568)              256800    \n",
      "=================================================================\n",
      "Total params: 513,600\n",
      "Trainable params: 513,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embeddding_size = 100\n",
    "skipgram = Sequential([\n",
    "    Embedding(voc_size, embeddding_size, input_length=1),\n",
    "    Flatten(),\n",
    "    Dense(voc_size, activation=\"softmax\", use_bias=False, kernel_regularizer=\"l2\")\n",
    "])\n",
    "\n",
    "skipgram.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 301109 samples\n",
      "Epoch 1/3\n",
      "301109/301109 [==============================] - 171s 566us/sample - loss: 6.8350 - accuracy: 0.0581\n",
      "Epoch 2/3\n",
      "301109/301109 [==============================] - 166s 551us/sample - loss: 6.4434 - accuracy: 0.0591\n",
      "Epoch 3/3\n",
      "301109/301109 [==============================] - 182s 603us/sample - loss: 6.3579 - accuracy: 0.0587\n"
     ]
    }
   ],
   "source": [
    "_ = skipgram.fit(x, y, batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the embedding layer from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = skipgram.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function to retrieve the embedding of a word. Can pass custom tokenizer or embedding_layer, but will otherwise use what we defined\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    word: str,\n",
    "    tokenizer: Tokenizer = tokenizer,\n",
    "    embedding_layer: Embedding = embedding_layer\n",
    ") -> np.ndarray:\n",
    "    word_index = tokenizer.texts_to_sequences([word])\n",
    "    output_tensor = embedding_layer(np.array(word_index))\n",
    "    return tf.reshape(output_tensor, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that retrieves the closes word to a word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(\n",
    "    v: Union[np.ndarray, tf.Tensor],\n",
    "    w: Union[np.ndarray, tf.Tensor]) -> float:\n",
    "    \"\"\"v and w should be 1D-vectors, for which to calculate the cosine similarity\"\"\"\n",
    "    \n",
    "    assert isinstance(v, tf.Tensor) or isinstance(v, np.ndarray), \"v must be tf.Tensor or np.ndarray\"\n",
    "    assert isinstance(w, tf.Tensor) or isinstance(w, np.ndarray), \"w must be tf.Tensor or np.ndarray\"\n",
    "    \n",
    "    v = v.numpy() if isinstance(v, tf.Tensor) else v\n",
    "    w = w.numpy() if isinstance(w, tf.Tensor) else w\n",
    "    \n",
    "    if v.ndim > 1:\n",
    "        v = v.flatten()\n",
    "    if w.ndim > 1:\n",
    "        w = w.flatten()\n",
    "\n",
    "    return v @ w.T / (np.linalg.norm(v) * np.linalg.norm(w))\n",
    "    \n",
    "def closest_n_words(\n",
    "    embedding: tf.Tensor,\n",
    "    n: int = 1,\n",
    "    tokenizer: Tokenizer = tokenizer,\n",
    "    embedding_layer: Embedding = embedding_layer\n",
    ") -> List[str]:    \n",
    "    all_words = np.array(list(tokenizer.index_word.values()))\n",
    "    all_embeddings = np.array([embed(word).numpy() for word in all_words]).squeeze()\n",
    "    similarities = np.apply_along_axis(\n",
    "        lambda row: cosine_similarity(row, embedding),\n",
    "        1, all_embeddings\n",
    "    )\n",
    "    sorted_idx = np.argsort(similarities)\n",
    "    \n",
    "    return np.flip(all_words[sorted_idx[-n:]]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we might think that $e_\\text{king} - e_\\text{man} \\approx e_\\text{queen} - e_\\text{woman}$. So if we try and find the closest word to \n",
    "$e_\\text{king} - e_\\text{queen} + e_\\text{woman}$ it should be close to $e_\\text{man}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy = embed(\"king\") - embed(\"queen\") + embed(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your',\n",
       " 'are',\n",
       " 'please',\n",
       " 'cat',\n",
       " 'take',\n",
       " 'replied',\n",
       " 'caterpillar',\n",
       " 'remarked',\n",
       " 'let',\n",
       " 'explain']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_n_words(analogy, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98425084"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(analogy, embed(\"man\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('tue_ml': conda)",
   "language": "python",
   "name": "python38164bittuemlconda3c4abf61ec77409097c05df25a7e1ce6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
