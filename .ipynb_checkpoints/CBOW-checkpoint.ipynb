{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Layer, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW\n",
    "\n",
    "Continuous Bag of Words implementation. I hope it makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Read in alice text and then tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alice.txt\", \"r\") as f:\n",
    "    raw_corpus = f.readlines()\n",
    "    \n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' + '\\'')\n",
    "tokenizer.fit_on_texts(raw_corpus)\n",
    "corpus = sum([sequence for sequence in tokenizer.texts_to_sequences(raw_corpus) if len(sequence) > 0], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(corpus)\n",
    "voc_size = max(corpus) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27330, 2568)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_size, voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Our training task for CBOW is to guess a word based on the `window_size` words surrounding it either side.\n",
    "Let's generate a dataset of `(window_words, word)` with fixed length window_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "    corpus: List[int],\n",
    "    window_size: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    raw_x = []\n",
    "    raw_y = []\n",
    "    for word_idx, word in enumerate(corpus):\n",
    "        min_idx = max(0, word_idx - window_size)\n",
    "        max_idx = min(len(corpus), word_idx + window_size)\n",
    "        raw_x.append(\n",
    "            corpus[min_idx : word_idx] + corpus[word_idx + 1 : max_idx + 1]\n",
    "        )\n",
    "        raw_y.append(word)\n",
    "    x = pad_sequences(raw_x) # Pads all sequences to a fixed length\n",
    "    y = np.array(raw_y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "x, y = generate_dataset(corpus, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training\n",
    "\n",
    "We then define the CBOW model and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom `AverageLayer` to average out the embeddings of the window words output from the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AverageLayer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.math.reduce_mean(inputs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can construct the model itself. We can vary the size of the embedding, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 8, 100)            256800    \n",
      "_________________________________________________________________\n",
      "average_layer_2 (AverageLaye (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2568)              256800    \n",
      "=================================================================\n",
      "Total params: 513,600\n",
      "Trainable params: 513,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "cbow = Sequential([\n",
    "    Embedding(voc_size, embedding_size, mask_zero=True, input_length=2 * window_size),\n",
    "    AverageLayer(),\n",
    "    Dense(voc_size, activation=\"softmax\", use_bias=False)\n",
    "])\n",
    "cbow.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27330 samples\n",
      "Epoch 1/3\n",
      "27330/27330 [==============================] - 259s 9ms/sample - loss: 6.3709 - accuracy: 0.0627\n",
      "Epoch 2/3\n",
      "27330/27330 [==============================] - 279s 10ms/sample - loss: 5.8063 - accuracy: 0.0989\n",
      "Epoch 3/3\n",
      "27330/27330 [==============================] - 273s 10ms/sample - loss: 5.3857 - accuracy: 0.1475\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "_ = cbow.fit(x, y, batch_size=1, epochs=3) #, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get the embedding layer from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = cbow.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function to retrieve the embedding of a word. Can pass custom tokenizer or embedding_layer, but will otherwise use what we defined\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(\n",
    "    word: str,\n",
    "    tokenizer: Tokenizer = tokenizer,\n",
    "    embedding_layer: Embedding = embedding_layer\n",
    ") -> np.ndarray:\n",
    "    word_index = tokenizer.texts_to_sequences([word])\n",
    "    output_tensor = embedding_layer(np.array(word_index))\n",
    "    return tf.reshape(output_tensor, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that retrieves the closes word to a word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(\n",
    "    v: Union[np.ndarray, tf.Tensor],\n",
    "    w: Union[np.ndarray, tf.Tensor]) -> float:\n",
    "    \"\"\"v and w should be 1D-vectors, for which to calculate the cosine similarity\"\"\"\n",
    "    \n",
    "    assert isinstance(v, tf.Tensor) or isinstance(v, np.ndarray), \"v must be tf.Tensor or np.ndarray\"\n",
    "    assert isinstance(w, tf.Tensor) or isinstance(w, np.ndarray), \"w must be tf.Tensor or np.ndarray\"\n",
    "    \n",
    "    v = v.numpy() if isinstance(v, tf.Tensor) else v\n",
    "    w = w.numpy() if isinstance(w, tf.Tensor) else w\n",
    "    \n",
    "    if v.ndim > 1:\n",
    "        v = v.flatten()\n",
    "    if w.ndim > 1:\n",
    "        w = w.flatten()\n",
    "\n",
    "    return v @ w.T / (np.linalg.norm(v) * np.linalg.norm(w))\n",
    "    \n",
    "def closest_n_words(\n",
    "    embedding: tf.Tensor,\n",
    "    n: int = 1,\n",
    "    tokenizer: Tokenizer = tokenizer,\n",
    "    embedding_layer: Embedding = embedding_layer\n",
    ") -> List[str]:    \n",
    "    all_words = np.array(list(tokenizer.index_word.values()))\n",
    "    all_embeddings = np.array([embed(word).numpy() for word in all_words]).squeeze()\n",
    "    similarities = np.apply_along_axis(\n",
    "        lambda row: cosine_similarity(row, embedding),\n",
    "        1, all_embeddings\n",
    "    )\n",
    "    sorted_idx = np.argsort(similarities)\n",
    "    \n",
    "    return np.flip(all_words[sorted_idx[-n:]]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we might think that $e_\\text{king} - e_\\text{man} \\approx e_\\text{queen} - e_\\text{woman}$. So if we try and find the closest word to \n",
    "$e_\\text{king} - e_\\text{queen} + e_\\text{woman}$ it should be close to $e_\\text{man}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy = embed(\"king\") - embed(\"queen\") + embed(\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman',\n",
       " 'prize',\n",
       " 'curious',\n",
       " 'useful',\n",
       " 'whiting',\n",
       " 'askance',\n",
       " 'grin',\n",
       " 'measure',\n",
       " 'pity',\n",
       " 'plainly']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_n_words(analogy, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.441663"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(analogy, embed(\"man\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
